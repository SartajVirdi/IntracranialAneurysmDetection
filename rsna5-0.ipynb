{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca2a9be",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-10T19:41:16.701968Z",
     "iopub.status.busy": "2025-08-10T19:41:16.701721Z",
     "iopub.status.idle": "2025-08-10T19:42:30.068737Z",
     "shell.execute_reply": "2025-08-10T19:42:30.067904Z"
    },
    "papermill": {
     "duration": 73.371405,
     "end_time": "2025-08-10T19:42:30.070294",
     "exception": false,
     "start_time": "2025-08-10T19:41:16.698889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# RSNA IAD — Fast Profile (<=12h) — single model + 3-center bagging + TTA=2\n",
    "\n",
    "import os, gc, shutil, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from contextlib import contextmanager\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Core deps\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "# --- DL stack\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# --- Competition API\n",
    "import kaggle_evaluation.rsna_inference_server as rsna\n",
    "\n",
    "# ================= Speed knobs =================\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ================= Device =================\n",
    "def setup_device():\n",
    "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device = setup_device()\n",
    "\n",
    "# ================= Config =================\n",
    "ID_COL = \"SeriesInstanceUID\"\n",
    "LABEL_COLS = [\n",
    "    'Left Infraclinoid Internal Carotid Artery','Right Infraclinoid Internal Carotid Artery',\n",
    "    'Left Supraclinoid Internal Carotid Artery','Right Supraclinoid Internal Carotid Artery',\n",
    "    'Left Middle Cerebral Artery','Right Middle Cerebral Artery',\n",
    "    'Anterior Communicating Artery','Left Anterior Cerebral Artery','Right Anterior Cerebral Artery',\n",
    "    'Left Posterior Communicating Artery','Right Posterior Communicating Artery',\n",
    "    'Basilar Tip','Other Posterior Circulation','Aneurysm Present',\n",
    "]\n",
    "\n",
    "MODEL_PATHS = {\n",
    "    'tf_efficientnetv2_s': '/kaggle/input/rsna-iad-trained-models/models/tf_efficientnetv2_s_fold0_best.pth',\n",
    "    # keep others for future, but unused in fast profile\n",
    "    'convnext_small': '/kaggle/input/rsna-iad-trained-models/models/convnext_small_fold0_best.pth',\n",
    "    'swin_small_patch4_window7_224': '/kaggle/input/rsna-iad-trained-models/models/swin_small_patch4_window7_224_fold0_best.pth'\n",
    "}\n",
    "\n",
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        # FAST PROFILE: single model\n",
    "        self.model_selection = \"tf_efficientnetv2_s\"\n",
    "        self.use_ensemble    = False\n",
    "\n",
    "        # Image/volume\n",
    "        self.image_size  = 512\n",
    "        self.num_slices  = 32\n",
    "        self.use_window  = True\n",
    "\n",
    "        # Inference\n",
    "        self.batch_size  = 1\n",
    "        self.use_amp     = torch.cuda.is_available()\n",
    "        self.use_tta     = True\n",
    "        self.tta_n       = 2     # identity + hflip\n",
    "\n",
    "        # Bagging\n",
    "        self.slice_bags  = 3     # centers around mid: -2,0,+2\n",
    "        self.thickness   = 0.8   # slab ~80% around center for MIP/STD\n",
    "\n",
    "        # Housekeeping\n",
    "        self.cleanup_every = 20\n",
    "        self.max_retries   = 2\n",
    "        self.fallback_ok   = True\n",
    "\n",
    "        # Default windowing (fallback if tags absent)\n",
    "        self.windowing = {\n",
    "            'CT': (40,80), 'CTA': (50,350), 'MRA': (600,1200), 'MRI': (40,80), 'default': (40,80)\n",
    "        }\n",
    "\n",
    "CFG = InferenceConfig()\n",
    "\n",
    "# ================= Model =================\n",
    "class MultiBackboneModel(nn.Module):\n",
    "    def __init__(self, model_name: str, num_classes: int = 14,\n",
    "                 pretrained: bool = True, drop_rate: float = 0.0, drop_path_rate: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self._create_backbone(model_name, pretrained, drop_rate, drop_path_rate)\n",
    "        self._determine_feature_dimensions()\n",
    "        self._create_classifier(drop_rate)\n",
    "\n",
    "    def _create_backbone(self, name, pretrained, drop_rate, drop_path_rate):\n",
    "        kw = dict(pretrained=pretrained, in_chans=3, drop_rate=drop_rate, num_classes=0, global_pool='')\n",
    "        if 'swin' in name:\n",
    "            kw.update({'drop_path_rate': drop_path_rate, 'img_size': CFG.image_size})\n",
    "        elif 'convnext' in name:\n",
    "            kw['drop_path_rate'] = drop_path_rate\n",
    "        self.backbone = timm.create_model(name, **kw)\n",
    "\n",
    "    def _determine_feature_dimensions(self):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1,3,CFG.image_size,CFG.image_size)\n",
    "            f = self.backbone(x)\n",
    "            if f.ndim == 4:\n",
    "                self.num_features, self.needs_pool, self.needs_seq_pool = f.shape[1], True, False\n",
    "                self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "            elif f.ndim == 3:\n",
    "                self.num_features, self.needs_pool, self.needs_seq_pool = f.shape[-1], False, True\n",
    "            else:\n",
    "                self.num_features, self.needs_pool, self.needs_seq_pool = f.shape[1], False, False\n",
    "\n",
    "    def _create_classifier(self, drop_rate):\n",
    "        self.meta_fc = nn.Sequential(\n",
    "            nn.Linear(2,16), nn.ReLU(inplace=True), nn.Dropout(0.2),\n",
    "            nn.Linear(16,32), nn.ReLU(inplace=True), nn.Dropout(0.1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_features+32,512), nn.BatchNorm1d(512), nn.ReLU(inplace=True), nn.Dropout(drop_rate),\n",
    "            nn.Linear(512,256), nn.BatchNorm1d(256), nn.ReLU(inplace=True), nn.Dropout(drop_rate*0.5),\n",
    "            nn.Linear(256,self.num_classes)\n",
    "        )\n",
    "\n",
    "    def _pool_features(self, f):\n",
    "        if self.needs_pool: return self.global_pool(f).flatten(1)\n",
    "        if self.needs_seq_pool: return f.mean(1)\n",
    "        if f.ndim==4: return F.adaptive_avg_pool2d(f,1).flatten(1)\n",
    "        if f.ndim==3: return f.mean(1)\n",
    "        return f\n",
    "\n",
    "    def forward(self, image, meta):\n",
    "        imgf = self._pool_features(self.backbone(image))\n",
    "        metf = self.meta_fc(meta)\n",
    "        return self.classifier(torch.cat([imgf,metf],1))\n",
    "\n",
    "# ================= DICOM utils =================\n",
    "@contextmanager\n",
    "def dicom_error_handler(_):\n",
    "    try: yield\n",
    "    except Exception: raise\n",
    "\n",
    "def _is_dicom_file(p:str)->bool:\n",
    "    try:\n",
    "        with open(p,'rb') as f:\n",
    "            f.seek(128); return f.read(4)==b'DICM'\n",
    "    except: return False\n",
    "\n",
    "def _apply_rescale(img:np.ndarray, ds:pydicom.Dataset)->np.ndarray:\n",
    "    try:\n",
    "        slope = float(getattr(ds,'RescaleSlope',1.0))\n",
    "        intercept = float(getattr(ds,'RescaleIntercept',0.0))\n",
    "        return img*slope + intercept\n",
    "    except: return img\n",
    "\n",
    "def _get_window_from_ds(ds)->Tuple[float,float]:\n",
    "    # Prefer DICOM tags; fallback to modality defaults\n",
    "    try:\n",
    "        wc = ds.WindowCenter; ww = ds.WindowWidth\n",
    "        if isinstance(wc, pydicom.multival.MultiValue): wc = float(wc[0])\n",
    "        else: wc = float(wc)\n",
    "        if isinstance(ww, pydicom.multival.MultiValue): ww = float(ww[0])\n",
    "        else: ww = float(ww)\n",
    "        if ww <= 1: ww = 1.0\n",
    "        return wc, ww\n",
    "    except:\n",
    "        modality = getattr(ds,'Modality','CT')\n",
    "        return CFG.windowing.get(modality, CFG.windowing['default'])\n",
    "\n",
    "def _process_pixel_array(img:np.ndarray)->np.ndarray:\n",
    "    if img.ndim==3 and img.shape[-1]==3:\n",
    "        img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2GRAY).astype(np.float32)\n",
    "    elif img.ndim==3:  # multi-frame -> middle\n",
    "        img = img[img.shape[0]//2]\n",
    "    elif img.ndim>3:\n",
    "        img = img.reshape(img.shape[-2], img.shape[-1])\n",
    "    return img\n",
    "\n",
    "def _safe_age(ds):\n",
    "    try:\n",
    "        s = str(getattr(ds,'PatientAge','050Y'))[:3]\n",
    "        d = int(''.join([c for c in s if c.isdigit()]) or 50)\n",
    "        return max(0,min(d,120))\n",
    "    except: return 50\n",
    "\n",
    "def _safe_sex(ds):\n",
    "    try: return 1 if str(getattr(ds,'PatientSex','M')).upper().startswith('M') else 0\n",
    "    except: return 0\n",
    "\n",
    "def apply_dicom_windowing(img:np.ndarray, center:float, width:float)->np.ndarray:\n",
    "    imin, imax = center - width/2.0, center + width/2.0\n",
    "    img = np.clip(img, imin, imax)\n",
    "    img = (img - imin) / max(imax - imin, 1e-6)\n",
    "    return (img*255).astype(np.uint8)\n",
    "\n",
    "def process_dicom_series(series_path:str)->Tuple[np.ndarray, Dict]:\n",
    "    series_path = Path(series_path)\n",
    "    files = []\n",
    "    for root,_,fs in os.walk(series_path):\n",
    "        for fn in fs:\n",
    "            p = os.path.join(root, fn)\n",
    "            if fn.lower().endswith(('.dcm','.dicom')) or _is_dicom_file(p):\n",
    "                files.append(p)\n",
    "    if not files:\n",
    "        return _get_default_volume_and_metadata()\n",
    "\n",
    "    # Sort by ImagePositionPatient (z) else InstanceNumber\n",
    "    def sort_key(p):\n",
    "        try:\n",
    "            ds = pydicom.dcmread(p, stop_before_pixels=True, force=True)\n",
    "            ipp = getattr(ds,'ImagePositionPatient', None)\n",
    "            if ipp is not None and len(ipp)>=3:\n",
    "                return float(ipp[2])\n",
    "            return int(getattr(ds,'InstanceNumber',0))\n",
    "        except: return 0\n",
    "    files.sort(key=sort_key)\n",
    "\n",
    "    slices, meta, err = [], {}, 0\n",
    "    for i,p in enumerate(files):\n",
    "        try:\n",
    "            with dicom_error_handler(p):\n",
    "                ds = pydicom.dcmread(p, force=True)\n",
    "                img = _process_pixel_array(ds.pixel_array.astype(np.float32))\n",
    "                if i==0:\n",
    "                    meta = {'modality': getattr(ds,'Modality','CT'),\n",
    "                            'age': _safe_age(ds), 'sex': _safe_sex(ds)}\n",
    "                img = _apply_rescale(img, ds)\n",
    "                if CFG.use_window:\n",
    "                    c,w = _get_window_from_ds(ds)\n",
    "                    img = apply_dicom_windowing(img, c, w)\n",
    "                else:\n",
    "                    mn, mx = img.min(), img.max()\n",
    "                    img = ((img - mn)/max(mx-mn,1e-6)*255).astype(np.uint8)\n",
    "                img = cv2.resize(img, (CFG.image_size, CFG.image_size), interpolation=cv2.INTER_LINEAR)\n",
    "                slices.append(img)\n",
    "        except:\n",
    "            err += 1\n",
    "            if err > len(files)*0.5:\n",
    "                return _get_default_volume_and_metadata()\n",
    "\n",
    "    if not meta: meta = {'age':50,'sex':0,'modality':'CT'}\n",
    "    volume = _create_volume_from_slices(slices)\n",
    "    return volume, meta\n",
    "\n",
    "def _create_volume_from_slices(slices:List[np.ndarray])->np.ndarray:\n",
    "    if not slices:\n",
    "        return np.zeros((CFG.num_slices, CFG.image_size, CFG.image_size), np.uint8)\n",
    "    vol = np.asarray(slices)\n",
    "    n = CFG.num_slices\n",
    "    if len(vol) > n:\n",
    "        idx = np.linspace(0, len(vol)-1, n).astype(int)\n",
    "        vol = vol[idx]\n",
    "    elif len(vol) < n:\n",
    "        pad = n - len(vol)\n",
    "        if len(vol)==1: vol = np.repeat(vol, n, axis=0)\n",
    "        else: vol = np.pad(vol, ((0,pad),(0,0),(0,0)), mode='edge')\n",
    "    return vol\n",
    "\n",
    "def _get_default_volume_and_metadata():\n",
    "    return (np.zeros((CFG.num_slices, CFG.image_size, CFG.image_size), np.uint8),\n",
    "            {'age':50,'sex':0,'modality':'CT'})\n",
    "\n",
    "# ================= Transforms =================\n",
    "def get_inference_transform():\n",
    "    return A.Compose([A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225], max_pixel_value=255.0),\n",
    "                      ToTensorV2()])\n",
    "\n",
    "def get_tta_transforms():\n",
    "    base = [A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]), ToTensorV2()]\n",
    "    return [\n",
    "        A.Compose(base),                          # identity\n",
    "        A.Compose([A.HorizontalFlip(p=1.0)] + base),\n",
    "    ]\n",
    "\n",
    "MODELS: Dict[str, nn.Module] = {}\n",
    "TRANSFORM: Optional[A.Compose] = None\n",
    "TTA_TRANSFORMS: Optional[List[A.Compose]] = None\n",
    "_PRED_COUNT = 0\n",
    "\n",
    "# ================= Loading =================\n",
    "def _validate_model(m:nn.Module):\n",
    "    with torch.no_grad():\n",
    "        im = torch.randn(1,3,CFG.image_size,CFG.image_size, device=device).to(memory_format=torch.channels_last)\n",
    "        me = torch.randn(1,2, device=device)\n",
    "        out = m(im, me)\n",
    "        if out.shape != (1,len(LABEL_COLS)):\n",
    "            raise RuntimeError(f\"bad output {out.shape}\")\n",
    "\n",
    "def load_single_model(name, path)->nn.Module:\n",
    "    if not os.path.exists(path): raise FileNotFoundError(path)\n",
    "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
    "    tr = ckpt.get('training_config', {})\n",
    "    if 'image_size' in tr: CFG.image_size = tr['image_size']\n",
    "    m = MultiBackboneModel(name, num_classes=tr.get('num_classes',14),\n",
    "                           pretrained=False, drop_rate=0.0, drop_path_rate=0.0)\n",
    "    m.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "    m.to(device)\n",
    "    m.eval()\n",
    "    m.to(memory_format=torch.channels_last)\n",
    "    _validate_model(m)\n",
    "    return m\n",
    "\n",
    "def load_models():\n",
    "    global MODELS, TRANSFORM, TTA_TRANSFORMS\n",
    "    MODELS.clear()\n",
    "    name = CFG.model_selection\n",
    "    MODELS[name] = load_single_model(name, MODEL_PATHS[name])\n",
    "    TRANSFORM = get_inference_transform()\n",
    "    if CFG.use_tta: TTA_TRANSFORMS = get_tta_transforms()\n",
    "    _warmup_models()\n",
    "\n",
    "def _warmup_models():\n",
    "    try:\n",
    "        im = torch.randn(CFG.batch_size,3,CFG.image_size,CFG.image_size, device=device).to(memory_format=torch.channels_last)\n",
    "        me = torch.randn(CFG.batch_size,2, device=device)\n",
    "        with torch.no_grad():\n",
    "            for m in MODELS.values():\n",
    "                with autocast(enabled=CFG.use_amp):\n",
    "                    _ = m(im, me)  # single pass\n",
    "        del im, me\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ================= Prediction helpers =================\n",
    "def _prepare_metadata_tensor(meta:Dict)->torch.Tensor:\n",
    "    age = float(np.clip(meta.get('age',50)/100.0, 0.0, 1.2))\n",
    "    sex = float(np.clip(int(meta.get('sex',0)), 0, 1))\n",
    "    return torch.tensor([[age,sex]], dtype=torch.float32, device=device)\n",
    "\n",
    "def _create_multichannel_input_with_thickness(volume: np.ndarray, center_idx: int, frac: float) -> np.ndarray:\n",
    "    mid = int(np.clip(center_idx, 1, volume.shape[0]-2))\n",
    "    half = max(int(volume.shape[0]*frac/2), 1)\n",
    "    lo, hi = max(0, mid-half), min(volume.shape[0], mid+half+1)\n",
    "    slab = volume[lo:hi]\n",
    "    middle = volume[mid]\n",
    "    mip = slab.max(axis=0)\n",
    "    std = slab.astype(np.float32).std(axis=0)\n",
    "    if std.max() > std.min():\n",
    "        std = ((std - std.min()) / max(std.max() - std.min(),1e-6) * 255).astype(np.uint8)\n",
    "    else:\n",
    "        std = np.full_like(middle, 128, dtype=np.uint8)\n",
    "    return np.stack([middle, mip, std], -1)\n",
    "\n",
    "def _tta_predict(model, image_np, meta_tensor):\n",
    "    preds = []\n",
    "    if CFG.use_tta and TTA_TRANSFORMS:\n",
    "        for tfm in TTA_TRANSFORMS[:CFG.tta_n]:\n",
    "            t = tfm(image=image_np)['image'].unsqueeze(0).to(device, non_blocking=True)\n",
    "            t = t.to(memory_format=torch.channels_last)\n",
    "            with torch.no_grad(), autocast(enabled=CFG.use_amp):\n",
    "                o = model(t, meta_tensor)\n",
    "            preds.append(torch.sigmoid(o).float().cpu().numpy())\n",
    "        return np.mean(preds,0).squeeze()\n",
    "    else:\n",
    "        t = TRANSFORM(image=image_np)['image'].unsqueeze(0).to(device, non_blocking=True)\n",
    "        t = t.to(memory_format=torch.channels_last)\n",
    "        with torch.no_grad(), autocast(enabled=CFG.use_amp):\n",
    "            o = model(t, meta_tensor)\n",
    "        return torch.sigmoid(o).float().cpu().numpy().squeeze()\n",
    "\n",
    "def predict_single_model(model: nn.Module, volume: np.ndarray, meta_tensor: torch.Tensor) -> np.ndarray:\n",
    "    centers = np.linspace(volume.shape[0]//2 - 2, volume.shape[0]//2 + 2, CFG.slice_bags).astype(int)\n",
    "    bag = []\n",
    "    for c in centers:\n",
    "        img = _create_multichannel_input_with_thickness(volume, c, CFG.thickness)\n",
    "        bag.append(_tta_predict(model, img, meta_tensor))\n",
    "    return np.mean(bag, axis=0)\n",
    "\n",
    "def _validate_predictions(p:np.ndarray)->np.ndarray:\n",
    "    if p.shape!=(len(LABEL_COLS),): p = np.resize(p, len(LABEL_COLS))\n",
    "    p = np.nan_to_num(p, nan=0.1, posinf=0.9, neginf=0.0)\n",
    "    return np.clip(p, 1e-3, 1-1e-3)\n",
    "\n",
    "# ================= Orchestration =================\n",
    "_MODELS_LOADED = False\n",
    "_COUNTER = 0\n",
    "\n",
    "def _manage_memory():\n",
    "    global _COUNTER\n",
    "    _COUNTER += 1\n",
    "    if _COUNTER % CFG.cleanup_every == 0:\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def _predict_inner(series_path: str) -> pl.DataFrame:\n",
    "    global _MODELS_LOADED\n",
    "    if not _MODELS_LOADED:\n",
    "        load_models()\n",
    "        _MODELS_LOADED = True\n",
    "\n",
    "    vol, meta = process_dicom_series(series_path)\n",
    "    meta_t = _prepare_metadata_tensor(meta)\n",
    "    model = list(MODELS.values())[0]\n",
    "    pred = predict_single_model(model, vol, meta_t)\n",
    "    pred = _validate_predictions(pred)\n",
    "    _manage_memory()\n",
    "    return pl.DataFrame(data=[pred.tolist()], schema=LABEL_COLS, orient='row')\n",
    "\n",
    "def _create_fallback_predictions()->pl.DataFrame:\n",
    "    vals = [0.05]*(len(LABEL_COLS)-1)+[0.1]\n",
    "    return pl.DataFrame(data=[vals], schema=LABEL_COLS, orient='row')\n",
    "\n",
    "def predict(series_path: str) -> pl.DataFrame:\n",
    "    try:\n",
    "        if not os.path.exists(series_path):\n",
    "            return _create_fallback_predictions()\n",
    "        return _predict_inner(series_path)\n",
    "    except Exception:\n",
    "        return _create_fallback_predictions()\n",
    "    finally:\n",
    "        try:\n",
    "            shared = '/kaggle/shared'\n",
    "            if os.path.exists(shared): shutil.rmtree(shared, ignore_errors=True)\n",
    "            os.makedirs(shared, exist_ok=True)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache(); torch.cuda.synchronize()\n",
    "            gc.collect()\n",
    "        except: pass\n",
    "\n",
    "# ================= Main (quiet) =================\n",
    "def main():\n",
    "    # Do not print; Kaggle will handle outputs/submission.parquet\n",
    "    server = rsna.RSNAInferenceServer(predict)\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        server.serve()\n",
    "    else:\n",
    "        server.run_local_gateway()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13190393,
     "isSourceIdPinned": false,
     "sourceId": 99552,
     "sourceType": "competition"
    },
    {
     "datasetId": 7976292,
     "sourceId": 12687919,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 80.243397,
   "end_time": "2025-08-10T19:42:32.831045",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-10T19:41:12.587648",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
