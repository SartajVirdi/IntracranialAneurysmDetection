{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca5c5e0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-07T07:19:56.401471Z",
     "iopub.status.busy": "2025-08-07T07:19:56.401227Z",
     "iopub.status.idle": "2025-08-07T07:21:30.986184Z",
     "shell.execute_reply": "2025-08-07T07:21:30.985370Z"
    },
    "papermill": {
     "duration": 94.590214,
     "end_time": "2025-08-07T07:21:30.987884",
     "exception": false,
     "start_time": "2025-08-07T07:19:56.397670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing compatible enhanced RSNA aneurysm detection system...\n",
      "Loading models...\n",
      "Loading tf_efficientnetv2_s from /kaggle/input/rsna-iad-trained-models/models/tf_efficientnetv2_s_fold0_best.pth...\n",
      "Model tf_efficientnetv2_s: detected 1280 features, output shape: torch.Size([1, 1280, 16, 16])\n",
      "Successfully loaded tf_efficientnetv2_s with best score: 0.6514184588787864\n",
      "Loading convnext_small from /kaggle/input/rsna-iad-trained-models/models/convnext_small_fold0_best.pth...\n",
      "Model convnext_small: detected 768 features, output shape: torch.Size([1, 768, 16, 16])\n",
      "Successfully loaded convnext_small with best score: 0.5659249983062382\n",
      "Loading swin_small_patch4_window7_224 from /kaggle/input/rsna-iad-trained-models/models/swin_small_patch4_window7_224_fold0_best.pth...\n",
      "Model swin_small_patch4_window7_224: detected 16 features, output shape: torch.Size([1, 16, 16, 768])\n",
      "Successfully loaded swin_small_patch4_window7_224 with best score: 0.5642581777046055\n",
      "Adjusted ensemble weights: {'tf_efficientnetv2_s': 0.4, 'convnext_small': 0.3, 'swin_small_patch4_window7_224': 0.3}\n",
      "Models loaded: ['tf_efficientnetv2_s', 'convnext_small', 'swin_small_patch4_window7_224']\n",
      "Warming up models...\n",
      "Ready for inference!\n",
      "Running in local development mode...\n",
      "Submission shape: (3, 15)\n",
      "Sample predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>SeriesInstanceUID</th><th>Left Infraclinoid Internal Carotid Artery</th><th>Right Infraclinoid Internal Carotid Artery</th><th>Left Supraclinoid Internal Carotid Artery</th><th>Right Supraclinoid Internal Carotid Artery</th><th>Left Middle Cerebral Artery</th><th>Right Middle Cerebral Artery</th><th>Anterior Communicating Artery</th><th>Left Anterior Cerebral Artery</th><th>Right Anterior Cerebral Artery</th><th>Left Posterior Communicating Artery</th><th>Right Posterior Communicating Artery</th><th>Basilar Tip</th><th>Other Posterior Circulation</th><th>Aneurysm Present</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;1.2.826.0.1.3680043.8.498.1007…</td><td>0.031416</td><td>0.031133</td><td>0.071234</td><td>0.06178</td><td>0.058167</td><td>0.061731</td><td>0.096985</td><td>0.01927</td><td>0.022185</td><td>0.02244</td><td>0.036353</td><td>0.031592</td><td>0.042471</td><td>0.305762</td></tr><tr><td>&quot;1.2.826.0.1.3680043.8.498.1002…</td><td>0.058258</td><td>0.058687</td><td>0.132593</td><td>0.127539</td><td>0.095251</td><td>0.108313</td><td>0.162329</td><td>0.036256</td><td>0.051248</td><td>0.051649</td><td>0.070337</td><td>0.05932</td><td>0.073285</td><td>0.531934</td></tr><tr><td>&quot;1.2.826.0.1.3680043.8.498.1005…</td><td>0.045877</td><td>0.057028</td><td>0.112964</td><td>0.106055</td><td>0.101288</td><td>0.112701</td><td>0.173907</td><td>0.033905</td><td>0.040761</td><td>0.048334</td><td>0.076495</td><td>0.051947</td><td>0.068365</td><td>0.549927</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 15)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ SeriesIns ┆ Left Infr ┆ Right Inf ┆ Left Supr ┆ … ┆ Right     ┆ Basilar   ┆ Other     ┆ Aneurysm │\n",
       "│ tanceUID  ┆ aclinoid  ┆ raclinoid ┆ aclinoid  ┆   ┆ Posterior ┆ Tip       ┆ Posterior ┆ Present  │\n",
       "│ ---       ┆ Internal  ┆ Internal  ┆ Internal  ┆   ┆ Communica ┆ ---       ┆ Circulati ┆ ---      │\n",
       "│ str       ┆ Car…      ┆ Ca…       ┆ Car…      ┆   ┆ ting …    ┆ f64       ┆ on        ┆ f64      │\n",
       "│           ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆           ┆ ---       ┆          │\n",
       "│           ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆           ┆ f64       ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 1.2.826.0 ┆ 0.031416  ┆ 0.031133  ┆ 0.071234  ┆ … ┆ 0.036353  ┆ 0.031592  ┆ 0.042471  ┆ 0.305762 │\n",
       "│ .1.368004 ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3.8.498.1 ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 007…      ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 1.2.826.0 ┆ 0.058258  ┆ 0.058687  ┆ 0.132593  ┆ … ┆ 0.070337  ┆ 0.05932   ┆ 0.073285  ┆ 0.531934 │\n",
       "│ .1.368004 ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3.8.498.1 ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 002…      ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 1.2.826.0 ┆ 0.045877  ┆ 0.057028  ┆ 0.112964  ┆ … ┆ 0.076495  ┆ 0.051947  ┆ 0.068365  ┆ 0.549927 │\n",
       "│ .1.368004 ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3.8.498.1 ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 005…      ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Statistics:\n",
      "Left Infraclinoid Internal Carotid Artery: 0.0452\n",
      "Right Infraclinoid Internal Carotid Artery: 0.0489\n",
      "Left Supraclinoid Internal Carotid Artery: 0.1056\n",
      "Right Supraclinoid Internal Carotid Artery: 0.0985\n",
      "Left Middle Cerebral Artery: 0.0849\n",
      "Right Middle Cerebral Artery: 0.0942\n",
      "Anterior Communicating Artery: 0.1444\n",
      "Left Anterior Cerebral Artery: 0.0298\n",
      "Right Anterior Cerebral Artery: 0.0381\n",
      "Left Posterior Communicating Artery: 0.0408\n",
      "Right Posterior Communicating Artery: 0.0611\n",
      "Basilar Tip: 0.0476\n",
      "Other Posterior Circulation: 0.0614\n",
      "Aneurysm Present: 0.4625\n",
      "Compatible enhanced RSNA inference system ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from IPython.display import display\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "# Medical imaging\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "# ML/DL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "import timm\n",
    "\n",
    "# Transformations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Competition API\n",
    "import kaggle_evaluation.rsna_inference_server\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Competition constants\n",
    "ID_COL = 'SeriesInstanceUID'\n",
    "LABEL_COLS = [\n",
    "    'Left Infraclinoid Internal Carotid Artery',\n",
    "    'Right Infraclinoid Internal Carotid Artery',\n",
    "    'Left Supraclinoid Internal Carotid Artery',\n",
    "    'Right Supraclinoid Internal Carotid Artery',\n",
    "    'Left Middle Cerebral Artery',\n",
    "    'Right Middle Cerebral Artery',\n",
    "    'Anterior Communicating Artery',\n",
    "    'Left Anterior Cerebral Artery',\n",
    "    'Right Anterior Cerebral Artery',\n",
    "    'Left Posterior Communicating Artery',\n",
    "    'Right Posterior Communicating Artery',\n",
    "    'Basilar Tip',\n",
    "    'Other Posterior Circulation',\n",
    "    'Aneurysm Present',\n",
    "]\n",
    "\n",
    "# FIXED: Use original model selection but keep enhancements in other areas\n",
    "SELECTED_MODEL = 'ensemble'\n",
    "\n",
    "MODEL_PATHS = {\n",
    "    'tf_efficientnetv2_s': '/kaggle/input/rsna-iad-trained-models/models/tf_efficientnetv2_s_fold0_best.pth',\n",
    "    'convnext_small': '/kaggle/input/rsna-iad-trained-models/models/convnext_small_fold0_best.pth',\n",
    "    'swin_small_patch4_window7_224': '/kaggle/input/rsna-iad-trained-models/models/swin_small_patch4_window7_224_fold0_best.pth'\n",
    "}\n",
    "\n",
    "class InferenceConfig:\n",
    "    # Model selection\n",
    "    model_selection = SELECTED_MODEL\n",
    "    use_ensemble = (SELECTED_MODEL == 'ensemble')\n",
    "    \n",
    "    # Default model settings\n",
    "    image_size = 512\n",
    "    num_slices = 32\n",
    "    use_windowing = True\n",
    "    \n",
    "    # Enhanced inference settings (kept from improvements)\n",
    "    batch_size = 2\n",
    "    use_amp = True\n",
    "    use_tta = True\n",
    "    tta_transforms = 6\n",
    "    \n",
    "    # Enhanced processing flags\n",
    "    use_modality_specific_channels = True\n",
    "    use_enhanced_preprocessing = True\n",
    "    \n",
    "    # Ensemble weights\n",
    "    ensemble_weights = {\n",
    "        'tf_efficientnetv2_s': 0.4,\n",
    "        'convnext_small': 0.3,\n",
    "        'swin_small_patch4_window7_224': 0.3\n",
    "    }\n",
    "\n",
    "CFG = InferenceConfig()\n",
    "\n",
    "class MultiBackboneModel(nn.Module):\n",
    "    \"\"\"FIXED: Compatible model that matches pretrained checkpoint structure\"\"\"\n",
    "    def __init__(self, model_name, num_classes=14, pretrained=True, \n",
    "                 drop_rate=0.3, drop_path_rate=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        \n",
    "        if 'swin' in model_name:\n",
    "            self.backbone = timm.create_model(\n",
    "                model_name, \n",
    "                pretrained=pretrained,\n",
    "                in_chans=3,\n",
    "                drop_rate=drop_rate,\n",
    "                drop_path_rate=drop_path_rate,\n",
    "                img_size=CFG.image_size,\n",
    "                num_classes=0,\n",
    "                global_pool=''\n",
    "            )\n",
    "        else:\n",
    "            self.backbone = timm.create_model(\n",
    "                model_name, \n",
    "                pretrained=pretrained,\n",
    "                in_chans=3,\n",
    "                drop_rate=drop_rate,\n",
    "                drop_path_rate=drop_path_rate,\n",
    "                num_classes=0,\n",
    "                global_pool=''\n",
    "            )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, CFG.image_size, CFG.image_size)\n",
    "            features = self.backbone(dummy_input)\n",
    "            \n",
    "            if len(features.shape) == 4:\n",
    "                num_features = features.shape[1]\n",
    "                self.needs_pool = True\n",
    "            elif len(features.shape) == 3:\n",
    "                num_features = features.shape[-1]\n",
    "                self.needs_pool = False\n",
    "                self.needs_seq_pool = True\n",
    "            else:\n",
    "                num_features = features.shape[1]\n",
    "                self.needs_pool = False\n",
    "                self.needs_seq_pool = False\n",
    "        \n",
    "        print(f\"Model {model_name}: detected {num_features} features, output shape: {features.shape}\")\n",
    "        \n",
    "        if self.needs_pool:\n",
    "            self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # FIXED: Use original metadata processing structure (2 inputs: age, sex)\n",
    "        self.meta_fc = nn.Sequential(\n",
    "            nn.Linear(2, 16),  # Back to original: age, sex only\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # FIXED: Use original classifier structure to match checkpoints\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features + 32, 512),  # Original size\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(512, 256),  # Original size\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(256, num_classes)  # Final layer\n",
    "        )\n",
    "        \n",
    "    def forward(self, image, meta):\n",
    "        # Extract image features\n",
    "        img_features = self.backbone(image)\n",
    "        \n",
    "        # Apply appropriate pooling based on model type\n",
    "        if hasattr(self, 'needs_pool') and self.needs_pool:\n",
    "            img_features = self.global_pool(img_features)\n",
    "            img_features = img_features.flatten(1)\n",
    "        elif hasattr(self, 'needs_seq_pool') and self.needs_seq_pool:\n",
    "            img_features = img_features.mean(dim=1)\n",
    "        elif len(img_features.shape) == 4:\n",
    "            img_features = F.adaptive_avg_pool2d(img_features, 1).flatten(1)\n",
    "        elif len(img_features.shape) == 3:\n",
    "            img_features = img_features.mean(dim=1)\n",
    "        \n",
    "        # Process metadata (back to original 2 features)\n",
    "        meta_features = self.meta_fc(meta)\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([img_features, meta_features], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(combined)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def apply_dicom_windowing(img: np.ndarray, window_center: float, window_width: float) -> np.ndarray:\n",
    "    \"\"\"Apply DICOM windowing\"\"\"\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    img = (img - img_min) / (img_max - img_min + 1e-7)\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "def get_windowing_params(modality: str) -> Tuple[float, float]:\n",
    "    \"\"\"Enhanced modality-specific windowing (kept from improvements)\"\"\"\n",
    "    windows = {\n",
    "        'CT': (40, 80),\n",
    "        'CTA': (50, 350),\n",
    "        'MRA': (600, 1200),\n",
    "        'MRI': (40, 80),\n",
    "        'TOF': (500, 1000),\n",
    "        'PC': (300, 600),\n",
    "    }\n",
    "    return windows.get(modality, (40, 80))\n",
    "\n",
    "def create_enhanced_channels(volume: np.ndarray, modality: str) -> np.ndarray:\n",
    "    \"\"\"Enhanced channel creation (kept from improvements)\"\"\"\n",
    "    if not CFG.use_modality_specific_channels:\n",
    "        # Original method\n",
    "        middle_slice = volume[CFG.num_slices // 2]\n",
    "        mip = np.max(volume, axis=0)\n",
    "        std_proj = np.std(volume, axis=0).astype(np.float32)\n",
    "    else:\n",
    "        # Enhanced method based on modality\n",
    "        if modality in ['CTA', 'MRA']:\n",
    "            # For angiographic data, use different projections\n",
    "            mip = np.max(volume, axis=0)\n",
    "            mean_proj = np.mean(volume, axis=0)\n",
    "            center_idx = CFG.num_slices // 2\n",
    "            middle_slice = volume[center_idx]\n",
    "        else:\n",
    "            # For CT/MRI, use original approach\n",
    "            center_idx = CFG.num_slices // 2\n",
    "            middle_slice = volume[center_idx]\n",
    "            mip = np.max(volume, axis=0)\n",
    "            mean_proj = np.mean(volume, axis=0)\n",
    "    \n",
    "    # Normalize projections\n",
    "    if CFG.use_modality_specific_channels and modality in ['CTA', 'MRA']:\n",
    "        # For angiographic data\n",
    "        if mean_proj.max() > mean_proj.min():\n",
    "            mean_proj = ((mean_proj - mean_proj.min()) / (mean_proj.max() - mean_proj.min()) * 255).astype(np.uint8)\n",
    "        else:\n",
    "            mean_proj = np.zeros_like(mean_proj, dtype=np.uint8)\n",
    "        \n",
    "        image = np.stack([middle_slice, mip, mean_proj], axis=-1)\n",
    "    else:\n",
    "        # Original approach with std projection\n",
    "        std_proj = np.std(volume, axis=0).astype(np.float32)\n",
    "        if std_proj.max() > std_proj.min():\n",
    "            std_proj = ((std_proj - std_proj.min()) / (std_proj.max() - std_proj.min()) * 255).astype(np.uint8)\n",
    "        else:\n",
    "            std_proj = np.zeros_like(std_proj, dtype=np.uint8)\n",
    "        \n",
    "        image = np.stack([middle_slice, mip, std_proj], axis=-1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def process_dicom_series(series_path: str) -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"Enhanced DICOM processing (kept improvements but simplified metadata)\"\"\"\n",
    "    series_path = Path(series_path)\n",
    "    \n",
    "    # Find all DICOM files\n",
    "    all_filepaths = []\n",
    "    for root, _, files in os.walk(series_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.dcm'):\n",
    "                all_filepaths.append(os.path.join(root, file))\n",
    "    all_filepaths.sort()\n",
    "    \n",
    "    if len(all_filepaths) == 0:\n",
    "        volume = np.zeros((CFG.num_slices, CFG.image_size, CFG.image_size), dtype=np.uint8)\n",
    "        metadata = {'age': 50, 'sex': 0, 'modality': 'CT'}\n",
    "        return volume, metadata\n",
    "    \n",
    "    # Process DICOM files\n",
    "    slices = []\n",
    "    metadata = {}\n",
    "    slice_positions = []\n",
    "    \n",
    "    for i, filepath in enumerate(all_filepaths):\n",
    "        try:\n",
    "            ds = pydicom.dcmread(filepath, force=True)\n",
    "            img = ds.pixel_array.astype(np.float32)\n",
    "            \n",
    "            # Handle multi-frame or color images\n",
    "            if img.ndim == 3:\n",
    "                if img.shape[-1] == 3:\n",
    "                    img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "                else:\n",
    "                    img = img[:, :, 0]\n",
    "            \n",
    "            # Enhanced slice position handling\n",
    "            try:\n",
    "                if hasattr(ds, 'SliceLocation'):\n",
    "                    slice_positions.append(float(ds.SliceLocation))\n",
    "                elif hasattr(ds, 'ImagePositionPatient'):\n",
    "                    slice_positions.append(float(ds.ImagePositionPatient[2]))\n",
    "                else:\n",
    "                    slice_positions.append(i)\n",
    "            except:\n",
    "                slice_positions.append(i)\n",
    "            \n",
    "            # Extract metadata from first file\n",
    "            if i == 0:\n",
    "                metadata['modality'] = getattr(ds, 'Modality', 'CT')\n",
    "                \n",
    "                try:\n",
    "                    age_str = getattr(ds, 'PatientAge', '050Y')\n",
    "                    age = int(''.join(filter(str.isdigit, age_str[:3])) or '50')\n",
    "                    metadata['age'] = min(age, 100)\n",
    "                except:\n",
    "                    metadata['age'] = 50\n",
    "                \n",
    "                try:\n",
    "                    sex = getattr(ds, 'PatientSex', 'M')\n",
    "                    metadata['sex'] = 1 if sex == 'M' else 0\n",
    "                except:\n",
    "                    metadata['sex'] = 0\n",
    "            \n",
    "            # Apply rescale if available\n",
    "            if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):\n",
    "                img = img * ds.RescaleSlope + ds.RescaleIntercept\n",
    "            \n",
    "            # Apply enhanced windowing\n",
    "            if CFG.use_windowing:\n",
    "                window_center, window_width = get_windowing_params(metadata['modality'])\n",
    "                img = apply_dicom_windowing(img, window_center, window_width)\n",
    "            else:\n",
    "                img_min, img_max = img.min(), img.max()\n",
    "                if img_max > img_min:\n",
    "                    img = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)\n",
    "                else:\n",
    "                    img = np.zeros_like(img, dtype=np.uint8)\n",
    "            \n",
    "            # Resize\n",
    "            img = cv2.resize(img, (CFG.image_size, CFG.image_size))\n",
    "            slices.append(img)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort slices by position if available\n",
    "    if len(slice_positions) == len(slices) and len(set(slice_positions)) > 1:\n",
    "        sorted_indices = np.argsort(slice_positions)\n",
    "        slices = [slices[i] for i in sorted_indices]\n",
    "    \n",
    "    # Enhanced slice sampling\n",
    "    if len(slices) == 0:\n",
    "        volume = np.zeros((CFG.num_slices, CFG.image_size, CFG.image_size), dtype=np.uint8)\n",
    "    else:\n",
    "        volume = np.array(slices)\n",
    "        if len(slices) > CFG.num_slices:\n",
    "            # Use center-focused sampling\n",
    "            start_idx = max(0, (len(slices) - CFG.num_slices) // 2)\n",
    "            volume = volume[start_idx:start_idx + CFG.num_slices]\n",
    "        elif len(slices) < CFG.num_slices:\n",
    "            pad_size = CFG.num_slices - len(slices)\n",
    "            volume = np.pad(volume, ((0, pad_size), (0, 0), (0, 0)), mode='edge')\n",
    "    \n",
    "    return volume, metadata\n",
    "\n",
    "def get_inference_transform():\n",
    "    \"\"\"Standard inference transformation\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_tta_transforms():\n",
    "    \"\"\"Enhanced TTA transforms (kept from improvements)\"\"\"\n",
    "    transforms = [\n",
    "        A.Compose([  # Original\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "        A.Compose([  # Horizontal flip\n",
    "            A.HorizontalFlip(p=1.0),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "        A.Compose([  # Vertical flip\n",
    "            A.VerticalFlip(p=1.0),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "        A.Compose([  # 90 degree rotation\n",
    "            A.RandomRotate90(p=1.0),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "        A.Compose([  # Brightness/Contrast\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "        A.Compose([  # Gaussian noise\n",
    "            A.GaussNoise(var_limit=(0.0, 0.01), p=1.0),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    ]\n",
    "    return transforms\n",
    "\n",
    "# Global variables\n",
    "MODELS = {}\n",
    "TRANSFORM = None\n",
    "TTA_TRANSFORMS = None\n",
    "\n",
    "def load_single_model(model_name: str, model_path: str) -> nn.Module:\n",
    "    \"\"\"FIXED: Load model with proper error handling\"\"\"\n",
    "    print(f\"Loading {model_name} from {model_path}...\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Warning: Model file not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        # Extract config\n",
    "        model_config = checkpoint.get('model_config', {})\n",
    "        training_config = checkpoint.get('training_config', {})\n",
    "        \n",
    "        # Update global config if needed\n",
    "        if 'image_size' in training_config:\n",
    "            CFG.image_size = training_config['image_size']\n",
    "        \n",
    "        # Initialize model with original architecture\n",
    "        model = MultiBackboneModel(\n",
    "            model_name=model_name,\n",
    "            num_classes=training_config.get('num_classes', 14),\n",
    "            pretrained=False,\n",
    "            drop_rate=0.0,\n",
    "            drop_path_rate=0.0\n",
    "        )\n",
    "        \n",
    "        # Load weights - should work now with matching architecture\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"Successfully loaded {model_name} with best score: {checkpoint.get('best_score', 'N/A')}\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load models with better fallback handling\"\"\"\n",
    "    global MODELS, TRANSFORM, TTA_TRANSFORMS\n",
    "    \n",
    "    print(\"Loading models...\")\n",
    "    \n",
    "    if CFG.use_ensemble:\n",
    "        # Load all available models for ensemble\n",
    "        loaded_models = {}\n",
    "        for model_name, model_path in MODEL_PATHS.items():\n",
    "            model = load_single_model(model_name, model_path)\n",
    "            if model is not None:\n",
    "                loaded_models[model_name] = model\n",
    "        \n",
    "        MODELS = loaded_models\n",
    "        \n",
    "        # Adjust ensemble weights for available models\n",
    "        if len(MODELS) > 0:\n",
    "            available_models = list(MODELS.keys())\n",
    "            total_weight = sum(CFG.ensemble_weights.get(name, 1.0) for name in available_models)\n",
    "            CFG.ensemble_weights = {name: CFG.ensemble_weights.get(name, 1.0) / total_weight \n",
    "                                   for name in available_models}\n",
    "            print(f\"Adjusted ensemble weights: {CFG.ensemble_weights}\")\n",
    "    else:\n",
    "        # Load single selected model\n",
    "        if CFG.model_selection in MODEL_PATHS:\n",
    "            model_path = MODEL_PATHS[CFG.model_selection]\n",
    "            model = load_single_model(CFG.model_selection, model_path)\n",
    "            if model is not None:\n",
    "                MODELS[CFG.model_selection] = model\n",
    "    \n",
    "    # Initialize transforms\n",
    "    TRANSFORM = get_inference_transform()\n",
    "    if CFG.use_tta:\n",
    "        TTA_TRANSFORMS = get_tta_transforms()\n",
    "    \n",
    "    print(f\"Models loaded: {list(MODELS.keys())}\")\n",
    "    \n",
    "    if len(MODELS) == 0:\n",
    "        print(\"Warning: No models could be loaded! Will use fallback predictions.\")\n",
    "        return\n",
    "    \n",
    "    # Warm up models\n",
    "    print(\"Warming up models...\")\n",
    "    dummy_image = torch.randn(1, 3, CFG.image_size, CFG.image_size).to(device)\n",
    "    dummy_meta = torch.randn(1, 2).to(device)  # Back to 2 features\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for model in MODELS.values():\n",
    "            _ = model(dummy_image, dummy_meta)\n",
    "    \n",
    "    print(\"Ready for inference!\")\n",
    "\n",
    "def predict_single_model(model: nn.Module, image: np.ndarray, meta_tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Make prediction with a single model\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    if CFG.use_tta and TTA_TRANSFORMS:\n",
    "        # Test time augmentation\n",
    "        for transform in TTA_TRANSFORMS[:CFG.tta_transforms]:\n",
    "            aug_image = transform(image=image)['image']\n",
    "            aug_image = aug_image.unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with autocast(enabled=CFG.use_amp):\n",
    "                    output = model(aug_image, meta_tensor)\n",
    "                    pred = torch.sigmoid(output)\n",
    "                    predictions.append(pred.cpu().numpy())\n",
    "        \n",
    "        # Average TTA predictions\n",
    "        return np.mean(predictions, axis=0).squeeze()\n",
    "    else:\n",
    "        # Single prediction\n",
    "        image_tensor = TRANSFORM(image=image)['image']\n",
    "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast(enabled=CFG.use_amp):\n",
    "                output = model(image_tensor, meta_tensor)\n",
    "                return torch.sigmoid(output).cpu().numpy().squeeze()\n",
    "\n",
    "def predict_ensemble(image: np.ndarray, meta_tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Make ensemble prediction\"\"\"\n",
    "    all_predictions = []\n",
    "    weights = []\n",
    "    \n",
    "    for model_name, model in MODELS.items():\n",
    "        pred = predict_single_model(model, image, meta_tensor)\n",
    "        all_predictions.append(pred)\n",
    "        weights.append(CFG.ensemble_weights.get(model_name, 1.0))\n",
    "    \n",
    "    # Weighted average\n",
    "    weights = np.array(weights) / np.sum(weights)\n",
    "    predictions = np.array(all_predictions)\n",
    "    \n",
    "    return np.average(predictions, weights=weights, axis=0)\n",
    "\n",
    "def _predict_inner(series_path: str) -> pl.DataFrame:\n",
    "    \"\"\"Enhanced prediction logic\"\"\"\n",
    "    global MODELS\n",
    "    \n",
    "    # Load models if not already loaded\n",
    "    if not MODELS:\n",
    "        load_models()\n",
    "    \n",
    "    # If still no models, use fallback\n",
    "    if len(MODELS) == 0:\n",
    "        print(\"No models available, using fallback predictions\")\n",
    "        return predict_fallback(series_path)\n",
    "    \n",
    "    # Extract series ID\n",
    "    series_id = os.path.basename(series_path)\n",
    "    \n",
    "    # Process DICOM series with enhancements\n",
    "    volume, metadata = process_dicom_series(series_path)\n",
    "    \n",
    "    # Create enhanced multi-channel input\n",
    "    image = create_enhanced_channels(volume, metadata['modality'])\n",
    "    \n",
    "    # Prepare metadata tensor (back to 2 features: age, sex)\n",
    "    age_normalized = metadata['age'] / 100.0\n",
    "    sex = metadata['sex']\n",
    "    meta_tensor = torch.tensor([[age_normalized, sex]], dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    if CFG.use_ensemble and len(MODELS) > 1:\n",
    "        final_pred = predict_ensemble(image, meta_tensor)\n",
    "    else:\n",
    "        # Use single model\n",
    "        model = list(MODELS.values())[0]\n",
    "        final_pred = predict_single_model(model, image, meta_tensor)\n",
    "    \n",
    "    # Create output dataframe\n",
    "    predictions_df = pl.DataFrame(\n",
    "        data=[final_pred.tolist()],\n",
    "        schema=LABEL_COLS,\n",
    "        orient='row'\n",
    "    )\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "def predict_fallback(series_path: str) -> pl.DataFrame:\n",
    "    \"\"\"Smart fallback predictions\"\"\"\n",
    "    # More informed conservative predictions based on medical knowledge\n",
    "    conservative_preds = [0.05, 0.05, 0.08, 0.08, 0.12, 0.12, 0.15, 0.08, 0.08, 0.06, 0.06, 0.18, 0.10, 0.08]\n",
    "    \n",
    "    predictions = pl.DataFrame(\n",
    "        data=[conservative_preds],\n",
    "        schema=LABEL_COLS,\n",
    "        orient='row'\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    shutil.rmtree('/kaggle/shared', ignore_errors=True)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def predict(series_path: str) -> pl.DataFrame:\n",
    "    \"\"\"Top-level prediction function with robust error handling\"\"\"\n",
    "    try:\n",
    "        return _predict_inner(series_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction for {os.path.basename(series_path)}: {e}\")\n",
    "        print(\"Using fallback predictions.\")\n",
    "        return predict_fallback(series_path)\n",
    "    finally:\n",
    "        # Enhanced cleanup\n",
    "        shared_dir = '/kaggle/shared'\n",
    "        shutil.rmtree(shared_dir, ignore_errors=True)\n",
    "        os.makedirs(shared_dir, exist_ok=True)\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "\n",
    "# Main execution\n",
    "print(\"Initializing compatible enhanced RSNA aneurysm detection system...\")\n",
    "load_models()\n",
    "\n",
    "# Initialize the inference server\n",
    "inference_server = kaggle_evaluation.rsna_inference_server.RSNAInferenceServer(predict)\n",
    "\n",
    "# Check if running in competition environment or local session\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(\"Running in competition mode...\")\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    print(\"Running in local development mode...\")\n",
    "    inference_server.run_local_gateway()\n",
    "    \n",
    "    # Display results if available\n",
    "    try:\n",
    "        submission_df = pl.read_parquet('/kaggle/working/submission.parquet')\n",
    "        print(f\"Submission shape: {submission_df.shape}\")\n",
    "        print(\"Sample predictions:\")\n",
    "        display(submission_df.head())\n",
    "        \n",
    "        # Add prediction statistics\n",
    "        print(\"\\nPrediction Statistics:\")\n",
    "        for i, col in enumerate(LABEL_COLS):\n",
    "            mean_pred = submission_df[col].mean()\n",
    "            print(f\"{col}: {mean_pred:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load submission file: {e}\")\n",
    "\n",
    "print(\"Compatible enhanced RSNA inference system ready!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13190393,
     "isSourceIdPinned": false,
     "sourceId": 99552,
     "sourceType": "competition"
    },
    {
     "datasetId": 7976292,
     "sourceId": 12687919,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 103.046726,
   "end_time": "2025-08-07T07:21:33.738167",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-07T07:19:50.691441",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
