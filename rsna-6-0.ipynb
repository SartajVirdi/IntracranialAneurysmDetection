{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ccf33c7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-11T10:04:14.729701Z",
     "iopub.status.busy": "2025-08-11T10:04:14.729311Z",
     "iopub.status.idle": "2025-08-11T10:05:32.532870Z",
     "shell.execute_reply": "2025-08-11T10:05:32.532075Z"
    },
    "papermill": {
     "duration": 77.808057,
     "end_time": "2025-08-11T10:05:32.534312",
     "exception": false,
     "start_time": "2025-08-11T10:04:14.726255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# RSNA IAD â€” Single-model (EffNetV2-S) + TTA=4, quiet, <=12h\n",
    "\n",
    "import os, gc, shutil, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from contextlib import contextmanager\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "# DL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "import timm\n",
    "\n",
    "# Augs\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Kaggle API\n",
    "import kaggle_evaluation.rsna_inference_server as rsna\n",
    "\n",
    "# -------- Speed knobs --------\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "QUIET = True  # keep outputs minimal\n",
    "\n",
    "def _log(msg):\n",
    "    if not QUIET:\n",
    "        print(msg)\n",
    "\n",
    "# -------- Device --------\n",
    "def setup_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = setup_device()\n",
    "\n",
    "# -------- Config --------\n",
    "ID_COL = 'SeriesInstanceUID'\n",
    "LABEL_COLS = [\n",
    "    'Left Infraclinoid Internal Carotid Artery','Right Infraclinoid Internal Carotid Artery',\n",
    "    'Left Supraclinoid Internal Carotid Artery','Right Supraclinoid Internal Carotid Artery',\n",
    "    'Left Middle Cerebral Artery','Right Middle Cerebral Artery',\n",
    "    'Anterior Communicating Artery','Left Anterior Cerebral Artery','Right Anterior Cerebral Artery',\n",
    "    'Left Posterior Communicating Artery','Right Posterior Communicating Artery',\n",
    "    'Basilar Tip','Other Posterior Circulation','Aneurysm Present',\n",
    "]\n",
    "\n",
    "MODEL_PATHS = {\n",
    "    'tf_efficientnetv2_s': '/kaggle/input/rsna-iad-trained-models/models/tf_efficientnetv2_s_fold0_best.pth',\n",
    "    'convnext_small': '/kaggle/input/rsna-iad-trained-models/models/convnext_small_fold0_best.pth',\n",
    "    'swin_small_patch4_window7_224': '/kaggle/input/rsna-iad-trained-models/models/swin_small_patch4_window7_224_fold0_best.pth'\n",
    "}\n",
    "\n",
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        self.model_selection = 'tf_efficientnetv2_s'\n",
    "        self.use_ensemble = False\n",
    "\n",
    "        self.image_size = 512\n",
    "        self.num_slices = 32\n",
    "        self.use_windowing = True\n",
    "\n",
    "        self.batch_size = 1\n",
    "        self.use_amp = torch.cuda.is_available()\n",
    "        self.use_tta = True\n",
    "        self.tta_transforms = 4  # (identity, hflip, vflip, small rotate)\n",
    "\n",
    "        self.enable_memory_cleanup = True\n",
    "        self.cleanup_frequency = 16\n",
    "        self.max_retries = 2\n",
    "        self.fallback_enabled = True\n",
    "\n",
    "        self.windowing_params = {\n",
    "            'CT': (40, 80),'CTA': (50, 350),'MRA': (600, 1200),'MRI': (40, 80),'default': (40, 80)\n",
    "        }\n",
    "\n",
    "CFG = InferenceConfig()\n",
    "\n",
    "# -------- Model --------\n",
    "class MultiBackboneModel(nn.Module):\n",
    "    def __init__(self, model_name: str, num_classes: int = 14,\n",
    "                 pretrained: bool = True, drop_rate: float = 0.0, drop_path_rate: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self._create_backbone(model_name, pretrained, drop_rate, drop_path_rate)\n",
    "        self._determine_feature_dimensions()\n",
    "        self._create_classifier(drop_rate)\n",
    "\n",
    "    def _create_backbone(self, name, pretrained, drop_rate, drop_path_rate):\n",
    "        kw = dict(pretrained=pretrained, in_chans=3, drop_rate=drop_rate, num_classes=0, global_pool='')\n",
    "        if 'swin' in name:\n",
    "            kw.update({'drop_path_rate': drop_path_rate, 'img_size': CFG.image_size})\n",
    "        elif 'convnext' in name:\n",
    "            kw['drop_path_rate'] = drop_path_rate\n",
    "        self.backbone = timm.create_model(name, **kw)\n",
    "\n",
    "    def _determine_feature_dimensions(self):\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1,3,CFG.image_size,CFG.image_size)\n",
    "            f = self.backbone(dummy)\n",
    "            if f.ndim == 4:\n",
    "                self.num_features, self.needs_pool, self.needs_seq_pool = f.shape[1], True, False\n",
    "                self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "            elif f.ndim == 3:\n",
    "                self.num_features, self.needs_pool, self.needs_seq_pool = f.shape[-1], False, True\n",
    "            else:\n",
    "                self.num_features, self.needs_pool, self.needs_seq_pool = f.shape[1], False, False\n",
    "\n",
    "    def _create_classifier(self, drop_rate):\n",
    "        self.meta_fc = nn.Sequential(\n",
    "            nn.Linear(2,16), nn.ReLU(inplace=True), nn.Dropout(0.2),\n",
    "            nn.Linear(16,32), nn.ReLU(inplace=True), nn.Dropout(0.1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_features+32,512), nn.BatchNorm1d(512), nn.ReLU(inplace=True), nn.Dropout(drop_rate),\n",
    "            nn.Linear(512,256), nn.BatchNorm1d(256), nn.ReLU(inplace=True), nn.Dropout(drop_rate*0.5),\n",
    "            nn.Linear(256,self.num_classes)\n",
    "        )\n",
    "\n",
    "    def _pool_features(self, f):\n",
    "        if self.needs_pool: return self.global_pool(f).flatten(1)\n",
    "        if self.needs_seq_pool: return f.mean(1)\n",
    "        if f.ndim==4: return F.adaptive_avg_pool2d(f,1).flatten(1)\n",
    "        if f.ndim==3: return f.mean(1)\n",
    "        return f\n",
    "\n",
    "    def forward(self, image: torch.Tensor, meta: torch.Tensor) -> torch.Tensor:\n",
    "        imgf = self._pool_features(self.backbone(image))\n",
    "        metf = self.meta_fc(meta)\n",
    "        return self.classifier(torch.cat([imgf,metf], dim=1))\n",
    "\n",
    "# -------- DICOM helpers --------\n",
    "@contextmanager\n",
    "def dicom_error_handler(_): \n",
    "    try: yield\n",
    "    except Exception: raise\n",
    "\n",
    "def _is_dicom_file(p:str)->bool:\n",
    "    try:\n",
    "        with open(p,'rb') as f:\n",
    "            f.seek(128); return f.read(4)==b'DICM'\n",
    "    except: return False\n",
    "\n",
    "def _apply_rescale(img:np.ndarray, ds:pydicom.Dataset)->np.ndarray:\n",
    "    try:\n",
    "        slope = float(getattr(ds,'RescaleSlope',1.0))\n",
    "        intercept = float(getattr(ds,'RescaleIntercept',0.0))\n",
    "        return img*slope + intercept\n",
    "    except: return img\n",
    "\n",
    "def _get_window_from_ds(ds)->Tuple[float,float]:\n",
    "    try:\n",
    "        wc = ds.WindowCenter; ww = ds.WindowWidth\n",
    "        if isinstance(wc, pydicom.multival.MultiValue): wc = float(wc[0])\n",
    "        else: wc = float(wc)\n",
    "        if isinstance(ww, pydicom.multival.MultiValue): ww = float(ww[0])\n",
    "        else: ww = float(ww)\n",
    "        if ww <= 1: ww = 1.0\n",
    "        return wc, ww\n",
    "    except:\n",
    "        modality = getattr(ds,'Modality','CT')\n",
    "        return CFG.windowing_params.get(modality, CFG.windowing_params['default'])\n",
    "\n",
    "def apply_dicom_windowing(img:np.ndarray, center:float, width:float)->np.ndarray:\n",
    "    imin, imax = center - width/2.0, center + width/2.0\n",
    "    img = np.clip(img, imin, imax)\n",
    "    img = (img - imin) / max(imax - imin, 1e-6)\n",
    "    return (img*255).astype(np.uint8)\n",
    "\n",
    "def _process_pixel_array(img: np.ndarray) -> np.ndarray:\n",
    "    if img.ndim==3 and img.shape[-1]==3:\n",
    "        img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2GRAY).astype(np.float32)\n",
    "    elif img.ndim==3:\n",
    "        img = img[img.shape[0]//2]\n",
    "    elif img.ndim>3:\n",
    "        img = img.reshape(img.shape[-2], img.shape[-1])\n",
    "    return img\n",
    "\n",
    "def _safe_age(ds):\n",
    "    try:\n",
    "        s = str(getattr(ds,'PatientAge','050Y'))[:3]\n",
    "        d = int(''.join([c for c in s if c.isdigit()]) or 50)\n",
    "        return max(0,min(d,120))\n",
    "    except: return 50\n",
    "\n",
    "def _safe_sex(ds):\n",
    "    try: return 1 if str(getattr(ds,'PatientSex','M')).upper().startswith('M') else 0\n",
    "    except: return 0\n",
    "\n",
    "def process_dicom_series(series_path: str) -> Tuple[np.ndarray, Dict]:\n",
    "    series_path = Path(series_path)\n",
    "    files = []\n",
    "    for root,_,fs in os.walk(series_path):\n",
    "        for fn in fs:\n",
    "            p = os.path.join(root, fn)\n",
    "            if fn.lower().endswith(('.dcm','.dicom')) or _is_dicom_file(p):\n",
    "                files.append(p)\n",
    "    if not files:\n",
    "        return _get_default_volume_and_metadata()\n",
    "\n",
    "    # sort by z (ImagePositionPatient[2]) then InstanceNumber fallback\n",
    "    def sort_key(p):\n",
    "        try:\n",
    "            ds = pydicom.dcmread(p, stop_before_pixels=True, force=True)\n",
    "            ipp = getattr(ds,'ImagePositionPatient', None)\n",
    "            if ipp is not None and len(ipp)>=3:\n",
    "                return float(ipp[2])\n",
    "            return int(getattr(ds,'InstanceNumber',0))\n",
    "        except: return 0\n",
    "    files.sort(key=sort_key)\n",
    "\n",
    "    slices, metadata, errors = [], {}, 0\n",
    "    for i,fp in enumerate(files):\n",
    "        try:\n",
    "            with dicom_error_handler(fp):\n",
    "                ds = pydicom.dcmread(fp, force=True)\n",
    "                img = _process_pixel_array(ds.pixel_array.astype(np.float32))\n",
    "                if i==0:\n",
    "                    metadata = {'modality': getattr(ds,'Modality','CT'),\n",
    "                                'age': _safe_age(ds), 'sex': _safe_sex(ds)}\n",
    "                img = _apply_rescale(img, ds)\n",
    "                if CFG.use_windowing:\n",
    "                    c,w = _get_window_from_ds(ds)\n",
    "                    img = apply_dicom_windowing(img, c, w)\n",
    "                else:\n",
    "                    mn, mx = img.min(), img.max()\n",
    "                    img = ((img-mn)/max(mx-mn,1e-6)*255).astype(np.uint8)\n",
    "                img = cv2.resize(img, (CFG.image_size, CFG.image_size), interpolation=cv2.INTER_LINEAR)\n",
    "                slices.append(img)\n",
    "        except Exception:\n",
    "            errors += 1\n",
    "            if errors > len(files)*0.5:\n",
    "                return _get_default_volume_and_metadata()\n",
    "\n",
    "    if not metadata:\n",
    "        metadata = {'age':50,'sex':0,'modality':'CT'}\n",
    "\n",
    "    volume = _create_volume_from_slices(slices)\n",
    "    return volume, metadata\n",
    "\n",
    "def _create_volume_from_slices(slices: List[np.ndarray]) -> np.ndarray:\n",
    "    if not slices:\n",
    "        return np.zeros((CFG.num_slices, CFG.image_size, CFG.image_size), np.uint8)\n",
    "    vol = np.asarray(slices)\n",
    "    n = CFG.num_slices\n",
    "    if len(vol) > n:\n",
    "        idx = np.linspace(0, len(vol)-1, n).astype(int)\n",
    "        vol = vol[idx]\n",
    "    elif len(vol) < n:\n",
    "        pad = n - len(vol)\n",
    "        if len(vol)==1:\n",
    "            vol = np.repeat(vol, n, axis=0)\n",
    "        else:\n",
    "            vol = np.pad(vol, ((0,pad),(0,0),(0,0)), mode='edge')\n",
    "    return vol\n",
    "\n",
    "def _get_default_volume_and_metadata():\n",
    "    return (np.zeros((CFG.num_slices, CFG.image_size, CFG.image_size), np.uint8),\n",
    "            {'age':50,'sex':0,'modality':'CT'})\n",
    "\n",
    "# -------- Transforms --------\n",
    "def get_inference_transform():\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225], max_pixel_value=255.0),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_tta_transforms():\n",
    "    base = [A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]), ToTensorV2()]\n",
    "    return [\n",
    "        A.Compose(base),                                  # identity\n",
    "        A.Compose([A.HorizontalFlip(p=1.0)] + base),      # hflip\n",
    "        A.Compose([A.VerticalFlip(p=1.0)] + base),        # vflip\n",
    "        A.Compose([A.Rotate(limit=15, p=1.0)] + base),    # small rotate\n",
    "    ]\n",
    "\n",
    "# -------- Globals --------\n",
    "MODELS: Dict[str, nn.Module] = {}\n",
    "TRANSFORM: Optional[A.Compose] = None\n",
    "TTA_TRANSFORMS: Optional[List[A.Compose]] = None\n",
    "PREDICTION_COUNT = 0\n",
    "\n",
    "# -------- Loading --------\n",
    "def _validate_model(model: nn.Module):\n",
    "    with torch.no_grad():\n",
    "        dummy_image = torch.randn(1,3,CFG.image_size,CFG.image_size, device=device).to(memory_format=torch.channels_last)\n",
    "        dummy_meta  = torch.randn(1,2, device=device)\n",
    "        out = model(dummy_image, dummy_meta)\n",
    "        if out.shape != (1, len(LABEL_COLS)):\n",
    "            raise RuntimeError(f\"Unexpected output shape: {out.shape}\")\n",
    "\n",
    "def load_single_model(model_name: str, model_path: str) -> nn.Module:\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Missing model: {model_path}\")\n",
    "    ckpt = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    tr = ckpt.get('training_config', {})\n",
    "    if 'image_size' in tr:\n",
    "        CFG.image_size = tr['image_size']\n",
    "    model = MultiBackboneModel(model_name, num_classes=tr.get('num_classes',14),\n",
    "                               pretrained=False, drop_rate=0.0, drop_path_rate=0.0)\n",
    "    try:\n",
    "        model.load_state_dict(ckpt['model_state_dict'], strict=True)\n",
    "    except RuntimeError:\n",
    "        model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    model.to(memory_format=torch.channels_last)\n",
    "    _validate_model(model)\n",
    "    return model\n",
    "\n",
    "def load_models():\n",
    "    global MODELS, TRANSFORM, TTA_TRANSFORMS\n",
    "    MODELS.clear()\n",
    "    name = CFG.model_selection\n",
    "    MODELS[name] = load_single_model(name, MODEL_PATHS[name])\n",
    "    TRANSFORM = get_inference_transform()\n",
    "    if CFG.use_tta:\n",
    "        TTA_TRANSFORMS = get_tta_transforms()\n",
    "    _warmup_models()\n",
    "\n",
    "def _warmup_models():\n",
    "    try:\n",
    "        x = torch.randn(CFG.batch_size,3,CFG.image_size,CFG.image_size, device=device).to(memory_format=torch.channels_last)\n",
    "        m = torch.randn(CFG.batch_size,2, device=device)\n",
    "        with torch.no_grad():\n",
    "            for model in MODELS.values():\n",
    "                with autocast(enabled=CFG.use_amp):\n",
    "                    _ = model(x,m)   # single pass warmup\n",
    "        del x, m\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# -------- Prediction --------\n",
    "def _create_multichannel_input(volume: np.ndarray) -> np.ndarray:\n",
    "    # middle slice + MIP + STD over full resampled stack (same as your 0.66 setup)\n",
    "    if volume.size == 0:\n",
    "        s = np.zeros((CFG.image_size, CFG.image_size), np.uint8)\n",
    "        return np.stack([s,s,s], -1)\n",
    "    mid = volume.shape[0] // 2\n",
    "    middle = volume[mid]\n",
    "    mip = np.max(volume, axis=0)\n",
    "    std = volume.astype(np.float32).std(axis=0)\n",
    "    if std.max() > std.min():\n",
    "        std = ((std - std.min()) / max(std.max()-std.min(),1e-6) * 255).astype(np.uint8)\n",
    "    else:\n",
    "        std = np.full_like(middle, 128, dtype=np.uint8)\n",
    "    img = np.stack([middle, mip, std], axis=-1)\n",
    "    return img\n",
    "\n",
    "def _prepare_metadata_tensor(metadata: Dict) -> torch.Tensor:\n",
    "    age = float(np.clip(metadata.get('age',50)/100.0, 0.0, 1.2))\n",
    "    sex = float(np.clip(int(metadata.get('sex',0)), 0, 1))\n",
    "    return torch.tensor([[age,sex]], dtype=torch.float32, device=device)\n",
    "\n",
    "def predict_single_model(model: nn.Module, image: np.ndarray, meta_tensor: torch.Tensor) -> np.ndarray:\n",
    "    preds = []\n",
    "    if CFG.use_tta and TTA_TRANSFORMS:\n",
    "        for tfm in TTA_TRANSFORMS[:CFG.tta_transforms]:\n",
    "            x = tfm(image=image)['image'].unsqueeze(0).to(device, non_blocking=True)\n",
    "            x = x.to(memory_format=torch.channels_last)\n",
    "            with torch.no_grad(), autocast(enabled=CFG.use_amp):\n",
    "                out = model(x, meta_tensor)\n",
    "            preds.append(torch.sigmoid(out).float().cpu().numpy())\n",
    "        return np.mean(preds, axis=0).squeeze()\n",
    "    else:\n",
    "        x = TRANSFORM(image=image)['image'].unsqueeze(0).to(device, non_blocking=True)\n",
    "        x = x.to(memory_format=torch.channels_last)\n",
    "        with torch.no_grad(), autocast(enabled=CFG.use_amp):\n",
    "            out = model(x, meta_tensor)\n",
    "        return torch.sigmoid(out).float().cpu().numpy().squeeze()\n",
    "\n",
    "def _validate_predictions(pred: np.ndarray) -> np.ndarray:\n",
    "    if pred.shape != (len(LABEL_COLS),):\n",
    "        pred = np.resize(pred, len(LABEL_COLS))\n",
    "    pred = np.nan_to_num(pred, nan=0.1, posinf=0.9, neginf=0.0)\n",
    "    return np.clip(pred, 1e-3, 1-1e-3)\n",
    "\n",
    "def _manage_memory():\n",
    "    global PREDICTION_COUNT\n",
    "    PREDICTION_COUNT += 1\n",
    "    if CFG.enable_memory_cleanup and PREDICTION_COUNT % CFG.cleanup_frequency == 0:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "\n",
    "def _predict_inner(series_path: str) -> pl.DataFrame:\n",
    "    global MODELS\n",
    "    if not MODELS:\n",
    "        load_models()\n",
    "    vol, meta = process_dicom_series(series_path)\n",
    "    img = _create_multichannel_input(vol)\n",
    "    meta_t = _prepare_metadata_tensor(meta)\n",
    "    model = MODELS[CFG.model_selection]\n",
    "    pred = predict_single_model(model, img, meta_t)\n",
    "    pred = _validate_predictions(pred)\n",
    "    _manage_memory()\n",
    "    return pl.DataFrame(data=[pred.tolist()], schema=LABEL_COLS, orient='row')\n",
    "\n",
    "def _create_fallback_predictions() -> pl.DataFrame:\n",
    "    vals = [0.05] * (len(LABEL_COLS)-1) + [0.1]\n",
    "    return pl.DataFrame(data=[vals], schema=LABEL_COLS, orient='row')\n",
    "\n",
    "def predict(series_path: str) -> pl.DataFrame:\n",
    "    try:\n",
    "        if not os.path.exists(series_path):\n",
    "            return _create_fallback_predictions()\n",
    "        return _predict_inner(series_path)\n",
    "    except Exception:\n",
    "        return _create_fallback_predictions()\n",
    "    finally:\n",
    "        try:\n",
    "            shared = '/kaggle/shared'\n",
    "            if os.path.exists(shared): shutil.rmtree(shared, ignore_errors=True)\n",
    "            os.makedirs(shared, exist_ok=True)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache(); torch.cuda.synchronize()\n",
    "            gc.collect()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# -------- Main --------\n",
    "def main():\n",
    "    if not QUIET:\n",
    "        print(\"Starting RSNA IAD inference (quiet mode=%s)\" % QUIET)\n",
    "    server = rsna.RSNAInferenceServer(predict)\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        server.serve()\n",
    "    else:\n",
    "        server.run_local_gateway()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13190393,
     "isSourceIdPinned": false,
     "sourceId": 99552,
     "sourceType": "competition"
    },
    {
     "datasetId": 7976292,
     "sourceId": 12687919,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 84.152536,
   "end_time": "2025-08-11T10:05:35.050289",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-11T10:04:10.897753",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
