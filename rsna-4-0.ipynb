{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81dd2433",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-10T05:09:58.287417Z",
     "iopub.status.busy": "2025-08-10T05:09:58.287129Z",
     "iopub.status.idle": "2025-08-10T05:11:55.280629Z",
     "shell.execute_reply": "2025-08-10T05:11:55.279966Z"
    },
    "papermill": {
     "duration": 116.99854,
     "end_time": "2025-08-10T05:11:55.282131",
     "exception": false,
     "start_time": "2025-08-10T05:09:58.283591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# RSNA IAD — Fast, Quiet, Ensemble Inference (Kaggle GPU)\n",
    "\n",
    "import os, gc, shutil, warnings, traceback\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from contextlib import contextmanager\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Core deps\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "# ---- DL stack\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ---- Competition API\n",
    "import kaggle_evaluation.rsna_inference_server as rsna\n",
    "\n",
    "# ===================== Speed knobs (Kaggle GPU) =====================\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# ===================== Device =====================\n",
    "def setup_device():\n",
    "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "device = setup_device()\n",
    "\n",
    "# ===================== Config =====================\n",
    "ID_COL = \"SeriesInstanceUID\"\n",
    "LABEL_COLS = [\n",
    "    'Left Infraclinoid Internal Carotid Artery','Right Infraclinoid Internal Carotid Artery',\n",
    "    'Left Supraclinoid Internal Carotid Artery','Right Supraclinoid Internal Carotid Artery',\n",
    "    'Left Middle Cerebral Artery','Right Middle Cerebral Artery',\n",
    "    'Anterior Communicating Artery','Left Anterior Cerebral Artery','Right Anterior Cerebral Artery',\n",
    "    'Left Posterior Communicating Artery','Right Posterior Communicating Artery',\n",
    "    'Basilar Tip','Other Posterior Circulation','Aneurysm Present',\n",
    "]\n",
    "\n",
    "MODEL_PATHS = {\n",
    "    'tf_efficientnetv2_s': '/kaggle/input/rsna-iad-trained-models/models/tf_efficientnetv2_s_fold0_best.pth',\n",
    "    'convnext_small': '/kaggle/input/rsna-iad-trained-models/models/convnext_small_fold0_best.pth',\n",
    "    'swin_small_patch4_window7_224': '/kaggle/input/rsna-iad-trained-models/models/swin_small_patch4_window7_224_fold0_best.pth'\n",
    "}\n",
    "\n",
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        # Use all three models\n",
    "        self.model_selection = \"ensemble\"\n",
    "        self.use_ensemble   = True\n",
    "\n",
    "        # Image / volume\n",
    "        self.image_size     = 512\n",
    "        self.num_slices     = 32\n",
    "        self.use_windowing  = True\n",
    "\n",
    "        # Inference\n",
    "        self.batch_size     = 1\n",
    "        self.use_amp        = torch.cuda.is_available()\n",
    "        self.use_tta        = True\n",
    "        self.tta_transforms = 6  # 4–8 is reasonable on Kaggle\n",
    "\n",
    "        # Bagging\n",
    "        self.slice_bags     = 5  # centers around mid\n",
    "        self.thickness_fracs = [1.0, 0.6]  # add 0.4 for more time\n",
    "\n",
    "        # Memory / errors\n",
    "        self.enable_memory_cleanup = True\n",
    "        self.cleanup_frequency = 12\n",
    "        self.max_retries = 2\n",
    "        self.fallback_enabled = True\n",
    "\n",
    "        # Ensemble weights\n",
    "        self.ensemble_weights = {\n",
    "            'tf_efficientnetv2_s': 0.40,\n",
    "            'convnext_small': 0.30,\n",
    "            'swin_small_patch4_window7_224': 0.30\n",
    "        }\n",
    "\n",
    "        # Default windowing\n",
    "        self.windowing_params = {\n",
    "            'CT': (40, 80),'CTA': (50, 350),'MRA': (600, 1200),'MRI': (40, 80),'default': (40, 80)\n",
    "        }\n",
    "\n",
    "CFG = InferenceConfig()\n",
    "\n",
    "# ===================== Model =====================\n",
    "class MultiBackboneModel(nn.Module):\n",
    "    def __init__(self, model_name: str, num_classes: int = 14,\n",
    "                 pretrained: bool = True, drop_rate: float = 0.0, drop_path_rate: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self._create_backbone(model_name, pretrained, drop_rate, drop_path_rate)\n",
    "        self._determine_feature_dimensions()\n",
    "        self._create_classifier(drop_rate)\n",
    "\n",
    "    def _create_backbone(self, model_name, pretrained, drop_rate, drop_path_rate):\n",
    "        kw = dict(pretrained=pretrained, in_chans=3, drop_rate=drop_rate,\n",
    "                  num_classes=0, global_pool='')\n",
    "        if 'swin' in model_name:\n",
    "            kw.update({'drop_path_rate': drop_path_rate, 'img_size': CFG.image_size})\n",
    "        elif 'convnext' in model_name:\n",
    "            kw['drop_path_rate'] = drop_path_rate\n",
    "        self.backbone = timm.create_model(model_name, **kw)\n",
    "\n",
    "    def _determine_feature_dimensions(self):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1,3,CFG.image_size,CFG.image_size)\n",
    "            f = self.backbone(x)\n",
    "            if f.ndim == 4:\n",
    "                self.num_features, self.needs_pool, self.needs_seq_pool = f.shape[1], True, False\n",
    "                self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "            elif f.ndim == 3:\n",
    "                self.num_features, self.needs_pool, self.needs_seq_pool = f.shape[-1], False, True\n",
    "            else:\n",
    "                self.num_features, self.needs_pool, self.needs_seq_pool = f.shape[1], False, False\n",
    "\n",
    "    def _create_classifier(self, drop_rate):\n",
    "        self.meta_fc = nn.Sequential(\n",
    "            nn.Linear(2,16), nn.ReLU(inplace=True), nn.Dropout(0.2),\n",
    "            nn.Linear(16,32), nn.ReLU(inplace=True), nn.Dropout(0.1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_features+32,512), nn.BatchNorm1d(512), nn.ReLU(inplace=True), nn.Dropout(drop_rate),\n",
    "            nn.Linear(512,256), nn.BatchNorm1d(256), nn.ReLU(inplace=True), nn.Dropout(drop_rate*0.5),\n",
    "            nn.Linear(256,self.num_classes)\n",
    "        )\n",
    "\n",
    "    def _pool_features(self, f):\n",
    "        if self.needs_pool: return self.global_pool(f).flatten(1)\n",
    "        if self.needs_seq_pool: return f.mean(1)\n",
    "        if f.ndim==4: return F.adaptive_avg_pool2d(f,1).flatten(1)\n",
    "        if f.ndim==3: return f.mean(1)\n",
    "        return f\n",
    "\n",
    "    def forward(self, image, meta):\n",
    "        imgf = self._pool_features(self.backbone(image))\n",
    "        metf = self.meta_fc(meta)\n",
    "        return self.classifier(torch.cat([imgf,metf],1))\n",
    "\n",
    "# ===================== DICOM utils =====================\n",
    "@contextmanager\n",
    "def dicom_error_handler(fp:str):\n",
    "    try: yield\n",
    "    except Exception: raise\n",
    "\n",
    "def _is_dicom_file(p:str)->bool:\n",
    "    try:\n",
    "        with open(p,'rb') as f:\n",
    "            f.seek(128); return f.read(4)==b'DICM'\n",
    "    except: return False\n",
    "\n",
    "def _apply_rescale(img:np.ndarray, ds:pydicom.Dataset)->np.ndarray:\n",
    "    try:\n",
    "        slope = float(getattr(ds,'RescaleSlope',1.0))\n",
    "        intercept = float(getattr(ds,'RescaleIntercept',0.0))\n",
    "        return img*slope + intercept\n",
    "    except: return img\n",
    "\n",
    "def _get_window_from_ds(ds)->Tuple[float,float]:\n",
    "    try:\n",
    "        wc = ds.WindowCenter; ww = ds.WindowWidth\n",
    "        if isinstance(wc, pydicom.multival.MultiValue): wc = float(wc[0])\n",
    "        else: wc = float(wc)\n",
    "        if isinstance(ww, pydicom.multival.MultiValue): ww = float(ww[0])\n",
    "        else: ww = float(ww)\n",
    "        if ww <= 1: ww = 1.0\n",
    "        return wc, ww\n",
    "    except:\n",
    "        modality = getattr(ds,'Modality','CT')\n",
    "        return CFG.windowing_params.get(modality, CFG.windowing_params['default'])\n",
    "\n",
    "def apply_dicom_windowing(img:np.ndarray, center:float, width:float)->np.ndarray:\n",
    "    imin, imax = center - width/2.0, center + width/2.0\n",
    "    img = np.clip(img, imin, imax)\n",
    "    img = (img - imin) / max(imax - imin, 1e-6)\n",
    "    return (img*255).astype(np.uint8)\n",
    "\n",
    "def _process_pixel_array(img:np.ndarray)->np.ndarray:\n",
    "    if img.ndim==3 and img.shape[-1]==3:\n",
    "        img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2GRAY).astype(np.float32)\n",
    "    elif img.ndim==3:  # multi-frame -> middle\n",
    "        img = img[img.shape[0]//2]\n",
    "    elif img.ndim>3:\n",
    "        img = img.reshape(img.shape[-2], img.shape[-1])\n",
    "    return img\n",
    "\n",
    "def _safe_age(ds):\n",
    "    try:\n",
    "        s = str(getattr(ds,'PatientAge','050Y'))[:3]\n",
    "        d = int(''.join([c for c in s if c.isdigit()]) or 50)\n",
    "        return max(0,min(d,120))\n",
    "    except: return 50\n",
    "\n",
    "def _safe_sex(ds):\n",
    "    try: return 1 if str(getattr(ds,'PatientSex','M')).upper().startswith('M') else 0\n",
    "    except: return 0\n",
    "\n",
    "def process_dicom_series(series_path:str)->Tuple[np.ndarray, Dict]:\n",
    "    series_path = Path(series_path)\n",
    "    files = []\n",
    "    for root,_,fs in os.walk(series_path):\n",
    "        for fn in fs:\n",
    "            p = os.path.join(root, fn)\n",
    "            if fn.lower().endswith(('.dcm','.dicom')) or _is_dicom_file(p):\n",
    "                files.append(p)\n",
    "    if not files:\n",
    "        return _get_default_volume_and_metadata()\n",
    "\n",
    "    # sort by ImagePositionPatient (z) else InstanceNumber\n",
    "    def sort_key(p):\n",
    "        try:\n",
    "            ds = pydicom.dcmread(p, stop_before_pixels=True, force=True)\n",
    "            ipp = getattr(ds,'ImagePositionPatient', None)\n",
    "            if ipp is not None and len(ipp)>=3:\n",
    "                return float(ipp[2])\n",
    "            return int(getattr(ds,'InstanceNumber',0))\n",
    "        except: return 0\n",
    "    files.sort(key=sort_key)\n",
    "\n",
    "    slices, meta, err = [], {}, 0\n",
    "    for i,p in enumerate(files):\n",
    "        try:\n",
    "            with dicom_error_handler(p):\n",
    "                ds = pydicom.dcmread(p, force=True)\n",
    "                img = _process_pixel_array(ds.pixel_array.astype(np.float32))\n",
    "                if i==0:\n",
    "                    meta = {'modality': getattr(ds,'Modality','CT'),\n",
    "                            'age': _safe_age(ds), 'sex': _safe_sex(ds)}\n",
    "                img = _apply_rescale(img, ds)\n",
    "                if CFG.use_windowing:\n",
    "                    c,w = _get_window_from_ds(ds)\n",
    "                    img = apply_dicom_windowing(img, c, w)\n",
    "                else:\n",
    "                    # fallback normalize\n",
    "                    mn, mx = img.min(), img.max()\n",
    "                    img = ((img - mn)/max(mx-mn,1e-6)*255).astype(np.uint8)\n",
    "                img = cv2.resize(img, (CFG.image_size, CFG.image_size), interpolation=cv2.INTER_LINEAR)\n",
    "                slices.append(img)\n",
    "        except:\n",
    "            err += 1\n",
    "            if err > len(files)*0.5:\n",
    "                return _get_default_volume_and_metadata()\n",
    "\n",
    "    if not meta: meta = {'age':50,'sex':0,'modality':'CT'}\n",
    "    volume = _create_volume_from_slices(slices)\n",
    "    return volume, meta\n",
    "\n",
    "def _create_volume_from_slices(slices:List[np.ndarray])->np.ndarray:\n",
    "    if not slices:\n",
    "        return np.zeros((CFG.num_slices, CFG.image_size, CFG.image_size), np.uint8)\n",
    "    vol = np.asarray(slices)\n",
    "    n = CFG.num_slices\n",
    "    if len(vol) > n:\n",
    "        idx = np.linspace(0, len(vol)-1, n).astype(int)\n",
    "        vol = vol[idx]\n",
    "    elif len(vol) < n:\n",
    "        pad = n - len(vol)\n",
    "        if len(vol)==1: vol = np.repeat(vol, n, axis=0)\n",
    "        else: vol = np.pad(vol, ((0,pad),(0,0),(0,0)), mode='edge')\n",
    "    return vol\n",
    "\n",
    "def _get_default_volume_and_metadata():\n",
    "    return (np.zeros((CFG.num_slices, CFG.image_size, CFG.image_size), np.uint8),\n",
    "            {'age':50,'sex':0,'modality':'CT'})\n",
    "\n",
    "# ===================== Transforms =====================\n",
    "def get_inference_transform():\n",
    "    return A.Compose([A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225], max_pixel_value=255.0),\n",
    "                      ToTensorV2()])\n",
    "def get_tta_transforms():\n",
    "    base = [A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]), ToTensorV2()]\n",
    "    return [\n",
    "        A.Compose(base),\n",
    "        A.Compose([A.HorizontalFlip(p=1.0)]+base),\n",
    "        A.Compose([A.VerticalFlip(p=1.0)]+base),\n",
    "        A.Compose([A.Transpose(p=1.0)]+base),\n",
    "        A.Compose([A.Rotate(limit=10,p=1.0)]+base),\n",
    "        A.Compose([A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=10, p=1.0)]+base),\n",
    "        # add more if you want CFG.tta_transforms > 6\n",
    "    ]\n",
    "\n",
    "MODELS: Dict[str, nn.Module] = {}\n",
    "TRANSFORM: Optional[A.Compose] = None\n",
    "TTA_TRANSFORMS: Optional[List[A.Compose]] = None\n",
    "PREDICTION_COUNT = 0\n",
    "\n",
    "# ===================== Loading =====================\n",
    "def _validate_model(m:nn.Module):\n",
    "    with torch.no_grad():\n",
    "        im = torch.randn(1,3,CFG.image_size,CFG.image_size, device=device).to(memory_format=torch.channels_last)\n",
    "        me = torch.randn(1,2, device=device)\n",
    "        out = m(im, me)\n",
    "        if out.shape != (1,len(LABEL_COLS)):\n",
    "            raise RuntimeError(f\"bad output {out.shape}\")\n",
    "\n",
    "def load_single_model(name, path)->nn.Module:\n",
    "    if not os.path.exists(path): raise FileNotFoundError(path)\n",
    "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
    "    tr = ckpt.get('training_config', {})\n",
    "    if 'image_size' in tr: CFG.image_size = tr['image_size']\n",
    "    m = MultiBackboneModel(name, num_classes=tr.get('num_classes',14),\n",
    "                           pretrained=False, drop_rate=0.0, drop_path_rate=0.0)\n",
    "    m.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "    m.to(device)\n",
    "    m.eval()\n",
    "    m.to(memory_format=torch.channels_last)\n",
    "    _validate_model(m)\n",
    "    return m\n",
    "\n",
    "def load_models():\n",
    "    global MODELS, TRANSFORM, TTA_TRANSFORMS\n",
    "    if CFG.use_ensemble:\n",
    "        for n,p in MODEL_PATHS.items():\n",
    "            try: MODELS[n] = load_single_model(n,p)\n",
    "            except: pass\n",
    "    else:\n",
    "        n = next(iter(MODEL_PATHS))\n",
    "        MODELS[n] = load_single_model(n, MODEL_PATHS[n])\n",
    "\n",
    "    TRANSFORM = get_inference_transform()\n",
    "    if CFG.use_tta: TTA_TRANSFORMS = get_tta_transforms()\n",
    "    _warmup_models()\n",
    "\n",
    "def _warmup_models():\n",
    "    try:\n",
    "        im = torch.randn(CFG.batch_size,3,CFG.image_size,CFG.image_size, device=device).to(memory_format=torch.channels_last)\n",
    "        me = torch.randn(CFG.batch_size,2, device=device)\n",
    "        with torch.no_grad():\n",
    "            for m in MODELS.values():\n",
    "                for _ in range(2):\n",
    "                    with autocast(enabled=CFG.use_amp):\n",
    "                        _ = m(im, me)\n",
    "        del im, me\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    except: pass\n",
    "\n",
    "# ===================== Blending helpers =====================\n",
    "def _to_logit(p, eps=1e-6):\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "def _from_logit(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def _blend_rank_guard(P: np.ndarray, W: np.ndarray) -> np.ndarray:\n",
    "    m, L = P.shape\n",
    "    ranks = np.argsort(np.argsort(P, axis=0), axis=0).astype(float)\n",
    "    ranks = (ranks / max(m-1, 1e-6))\n",
    "    return (ranks * W[:, None]).sum(axis=0)\n",
    "\n",
    "def _blend_predictions(pred_list: list[np.ndarray], weights: list[float]) -> np.ndarray:\n",
    "    P = np.vstack(pred_list)  # (m, L)\n",
    "    W = np.asarray(weights, dtype=float)\n",
    "    W = W / (W.sum() + 1e-12)\n",
    "\n",
    "    # logit mean\n",
    "    Z = _to_logit(P)\n",
    "    z = (Z * W[:, None]).sum(axis=0)\n",
    "    p_logit = _from_logit(z)\n",
    "\n",
    "    # rank mean\n",
    "    p_rank = _blend_rank_guard(P, W)\n",
    "\n",
    "    # choose rank when models collapse\n",
    "    colvar = P.var(axis=0)\n",
    "    choose_rank = colvar < 1e-5\n",
    "    out = np.where(choose_rank, p_rank, p_logit)\n",
    "    return out\n",
    "\n",
    "# ===================== Prediction =====================\n",
    "def _prepare_metadata_tensor(meta:Dict)->torch.Tensor:\n",
    "    age = float(np.clip(meta.get('age',50)/100.0, 0.0, 1.2))\n",
    "    sex = float(np.clip(int(meta.get('sex',0)), 0, 1))\n",
    "    return torch.tensor([[age,sex]], dtype=torch.float32, device=device)\n",
    "\n",
    "def _create_multichannel_input_with_thickness(volume: np.ndarray, center_idx: int, frac: float) -> np.ndarray:\n",
    "    mid = int(np.clip(center_idx, 1, volume.shape[0]-2))\n",
    "    half = max(int(volume.shape[0]*frac/2), 1)\n",
    "    lo, hi = max(0, mid-half), min(volume.shape[0], mid+half+1)\n",
    "    slab = volume[lo:hi]\n",
    "    middle = volume[mid]\n",
    "    mip = slab.max(axis=0)\n",
    "    std = slab.astype(np.float32).std(axis=0)\n",
    "    if std.max() > std.min():\n",
    "        std = ((std - std.min()) / max(std.max() - std.min(),1e-6) * 255).astype(np.uint8)\n",
    "    else:\n",
    "        std = np.full_like(middle, 128, dtype=np.uint8)\n",
    "    return np.stack([middle, mip, std], -1)\n",
    "\n",
    "def _tta_predict(model, image_np, meta_tensor):\n",
    "    preds = []\n",
    "    if CFG.use_tta and TTA_TRANSFORMS:\n",
    "        for tfm in TTA_TRANSFORMS[:CFG.tta_transforms]:\n",
    "            t = tfm(image=image_np)['image'].unsqueeze(0).to(device, non_blocking=True)\n",
    "            t = t.to(memory_format=torch.channels_last)\n",
    "            with torch.no_grad(), autocast(enabled=CFG.use_amp):\n",
    "                o = model(t, meta_tensor)\n",
    "            preds.append(torch.sigmoid(o).float().cpu().numpy())\n",
    "        return np.mean(preds,0).squeeze()\n",
    "    else:\n",
    "        t = TRANSFORM(image=image_np)['image'].unsqueeze(0).to(device, non_blocking=True)\n",
    "        t = t.to(memory_format=torch.channels_last)\n",
    "        with torch.no_grad(), autocast(enabled=CFG.use_amp):\n",
    "            o = model(t, meta_tensor)\n",
    "        return torch.sigmoid(o).float().cpu().numpy().squeeze()\n",
    "\n",
    "def predict_single_model(model: nn.Module, volume: np.ndarray, meta_tensor: torch.Tensor) -> np.ndarray:\n",
    "    centers = np.linspace(volume.shape[0]//2 - 3, volume.shape[0]//2 + 3, CFG.slice_bags).astype(int)\n",
    "    bag = []\n",
    "    for c in centers:\n",
    "        for frac in CFG.thickness_fracs:\n",
    "            img = _create_multichannel_input_with_thickness(volume, c, frac)\n",
    "            bag.append(_tta_predict(model, img, meta_tensor))\n",
    "    return np.mean(bag, axis=0)\n",
    "\n",
    "def predict_ensemble(volume:np.ndarray, meta_tensor:torch.Tensor)->np.ndarray:\n",
    "    preds, wts = [], []\n",
    "    for name, m in MODELS.items():\n",
    "        try:\n",
    "            preds.append(predict_single_model(m, volume, meta_tensor))\n",
    "            wts.append(CFG.ensemble_weights.get(name,1.0))\n",
    "        except:\n",
    "            pass\n",
    "    if not preds: return np.full(len(LABEL_COLS), 0.1)\n",
    "    return _blend_predictions(preds, wts)\n",
    "\n",
    "def _validate_predictions(p:np.ndarray)->np.ndarray:\n",
    "    if p.shape!=(len(LABEL_COLS),): p = np.resize(p, len(LABEL_COLS))\n",
    "    p = np.nan_to_num(p, nan=0.1, posinf=0.9, neginf=0.0)\n",
    "    return np.clip(p, 1e-3, 1-1e-3)\n",
    "\n",
    "def _manage_memory():\n",
    "    global PREDICTION_COUNT\n",
    "    PREDICTION_COUNT += 1\n",
    "    if CFG.enable_memory_cleanup and (PREDICTION_COUNT % CFG.cleanup_frequency == 0):\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def _predict_inner(series_path:str)->pl.DataFrame:\n",
    "    if not MODELS: load_models()\n",
    "    vol, meta = process_dicom_series(series_path)\n",
    "    meta_t = _prepare_metadata_tensor(meta)\n",
    "    if CFG.use_ensemble and len(MODELS)>1:\n",
    "        pred = predict_ensemble(vol, meta_t)\n",
    "    else:\n",
    "        model = list(MODELS.values())[0]\n",
    "        pred = predict_single_model(model, vol, meta_t)\n",
    "    pred = _validate_predictions(pred)\n",
    "    _manage_memory()\n",
    "    return pl.DataFrame(data=[pred.tolist()], schema=LABEL_COLS, orient='row')\n",
    "\n",
    "def _create_fallback_predictions()->pl.DataFrame:\n",
    "    vals = [0.05]*(len(LABEL_COLS)-1)+[0.1]\n",
    "    return pl.DataFrame(data=[vals], schema=LABEL_COLS, orient='row')\n",
    "\n",
    "def predict(series_path:str)->pl.DataFrame:\n",
    "    try:\n",
    "        if not os.path.exists(series_path):\n",
    "            return _create_fallback_predictions()\n",
    "        return _predict_inner(series_path)\n",
    "    except Exception:\n",
    "        return _create_fallback_predictions()\n",
    "    finally:\n",
    "        try:\n",
    "            shared = '/kaggle/shared'\n",
    "            if os.path.exists(shared): shutil.rmtree(shared, ignore_errors=True)\n",
    "            os.makedirs(shared, exist_ok=True)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache(); torch.cuda.synchronize()\n",
    "            gc.collect()\n",
    "        except: pass\n",
    "\n",
    "# ===================== Main (quiet) =====================\n",
    "def main():\n",
    "    load_models()\n",
    "    server = rsna.RSNAInferenceServer(predict)\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        server.serve()\n",
    "    else:\n",
    "        server.run_local_gateway()\n",
    "        # Quiet: no prints. Kaggle will save submission.parquet.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13190393,
     "isSourceIdPinned": false,
     "sourceId": 99552,
     "sourceType": "competition"
    },
    {
     "datasetId": 7976292,
     "sourceId": 12687919,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 126.209393,
   "end_time": "2025-08-10T05:11:58.422721",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-10T05:09:52.213328",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
