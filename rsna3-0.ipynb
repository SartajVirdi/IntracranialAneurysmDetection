{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324d540a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-09T08:52:13.456289Z",
     "iopub.status.busy": "2025-08-09T08:52:13.456095Z",
     "iopub.status.idle": "2025-08-09T08:53:44.071000Z",
     "shell.execute_reply": "2025-08-09T08:53:44.070206Z"
    },
    "papermill": {
     "duration": 90.619911,
     "end_time": "2025-08-09T08:53:44.072517",
     "exception": false,
     "start_time": "2025-08-09T08:52:13.452606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# RSNA IAD — Fast, Quiet, Ensemble Inference (Kaggle GPU)\n",
    "\n",
    "import os, gc, json, shutil, warnings, traceback\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from contextlib import contextmanager\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------- Core deps --------\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import kaggle_evaluation.rsna_inference_server as rsna\n",
    "\n",
    "# -------- Speed knobs (Kaggle GPU) --------\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# =============== Device ===============\n",
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = torch.device(\"cuda\")\n",
    "        # quiet: no printing here\n",
    "    else:\n",
    "        dev = torch.device(\"cpu\")\n",
    "    return dev\n",
    "\n",
    "device = setup_device()\n",
    "\n",
    "# =============== Config ===============\n",
    "ID_COL = \"SeriesInstanceUID\"\n",
    "LABEL_COLS = [\n",
    "    'Left Infraclinoid Internal Carotid Artery','Right Infraclinoid Internal Carotid Artery',\n",
    "    'Left Supraclinoid Internal Carotid Artery','Right Supraclinoid Internal Carotid Artery',\n",
    "    'Left Middle Cerebral Artery','Right Middle Cerebral Artery',\n",
    "    'Anterior Communicating Artery','Left Anterior Cerebral Artery','Right Anterior Cerebral Artery',\n",
    "    'Left Posterior Communicating Artery','Right Posterior Communicating Artery',\n",
    "    'Basilar Tip','Other Posterior Circulation','Aneurysm Present',\n",
    "]\n",
    "\n",
    "MODEL_PATHS = {\n",
    "    'tf_efficientnetv2_s': '/kaggle/input/rsna-iad-trained-models/models/tf_efficientnetv2_s_fold0_best.pth',\n",
    "    'convnext_small': '/kaggle/input/rsna-iad-trained-models/models/convnext_small_fold0_best.pth',\n",
    "    'swin_small_patch4_window7_224': '/kaggle/input/rsna-iad-trained-models/models/swin_small_patch4_window7_224_fold0_best.pth'\n",
    "}\n",
    "\n",
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        self.model_selection = \"ensemble\"  # <- enable ensemble for a boost\n",
    "        self.use_ensemble   = True\n",
    "        self.image_size     = 512\n",
    "        self.num_slices     = 32\n",
    "        self.use_windowing  = True\n",
    "\n",
    "        # Inference\n",
    "        self.batch_size     = 1\n",
    "        self.use_amp        = torch.cuda.is_available()\n",
    "        self.use_tta        = True\n",
    "        self.tta_transforms = 4\n",
    "\n",
    "        # Slice bagging: evaluate several centers and average\n",
    "        self.slice_bags     = 5   # try 3–7; small cost, nice gain\n",
    "\n",
    "        # Memory / errors\n",
    "        self.enable_memory_cleanup = True\n",
    "        self.cleanup_frequency = 12\n",
    "        self.max_retries = 2\n",
    "        self.fallback_enabled = True\n",
    "\n",
    "        self.ensemble_weights = {\n",
    "            'tf_efficientnetv2_s': 0.4,\n",
    "            'convnext_small': 0.3,\n",
    "            'swin_small_patch4_window7_224': 0.3\n",
    "        }\n",
    "\n",
    "        self.windowing_params = {\n",
    "            'CT': (40, 80),'CTA': (50, 350),'MRA': (600, 1200),'MRI': (40, 80),'default': (40, 80)\n",
    "        }\n",
    "CFG = InferenceConfig()\n",
    "\n",
    "# =============== Model ===============\n",
    "class MultiBackboneModel(nn.Module):\n",
    "    def __init__(self, model_name: str, num_classes: int = 14,\n",
    "                 pretrained: bool = True, drop_rate: float = 0.0, drop_path_rate: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self._create_backbone(model_name, pretrained, drop_rate, drop_path_rate)\n",
    "        self._determine_feature_dimensions()\n",
    "        self._create_classifier(drop_rate)\n",
    "\n",
    "    def _create_backbone(self, model_name, pretrained, drop_rate, drop_path_rate):\n",
    "        kw = dict(pretrained=pretrained, in_chans=3, drop_rate=drop_rate,\n",
    "                  num_classes=0, global_pool='')\n",
    "        if 'swin' in model_name:\n",
    "            kw.update({'drop_path_rate': drop_path_rate, 'img_size': CFG.image_size})\n",
    "        elif 'convnext' in model_name:\n",
    "            kw['drop_path_rate'] = drop_path_rate\n",
    "        self.backbone = timm.create_model(model_name, **kw)\n",
    "\n",
    "    def _determine_feature_dimensions(self):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1,3,CFG.image_size,CFG.image_size)\n",
    "            f = self.backbone(x)\n",
    "            if f.ndim == 4:\n",
    "                self.num_features, self.needs_pool, self.needs_seq_pool = f.shape[1], True, False\n",
    "                self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "            elif f.ndim == 3:\n",
    "                self.num_features, self.needs_pool, self.needs_seq_pool = f.shape[-1], False, True\n",
    "            else:\n",
    "                self.num_features, self.needs_pool, self.needs_seq_pool = f.shape[1], False, False\n",
    "\n",
    "    def _create_classifier(self, drop_rate):\n",
    "        self.meta_fc = nn.Sequential(\n",
    "            nn.Linear(2,16), nn.ReLU(inplace=True), nn.Dropout(0.2),\n",
    "            nn.Linear(16,32), nn.ReLU(inplace=True), nn.Dropout(0.1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_features+32,512), nn.BatchNorm1d(512), nn.ReLU(inplace=True), nn.Dropout(drop_rate),\n",
    "            nn.Linear(512,256), nn.BatchNorm1d(256), nn.ReLU(inplace=True), nn.Dropout(drop_rate*0.5),\n",
    "            nn.Linear(256,self.num_classes)\n",
    "        )\n",
    "\n",
    "    def _pool_features(self, f):\n",
    "        if self.needs_pool: return self.global_pool(f).flatten(1)\n",
    "        if self.needs_seq_pool: return f.mean(1)\n",
    "        if f.ndim==4: return F.adaptive_avg_pool2d(f,1).flatten(1)\n",
    "        if f.ndim==3: return f.mean(1)\n",
    "        return f\n",
    "\n",
    "    def forward(self, image, meta):\n",
    "        imgf = self._pool_features(self.backbone(image))\n",
    "        metf = self.meta_fc(meta)\n",
    "        return self.classifier(torch.cat([imgf,metf],1))\n",
    "\n",
    "# =============== DICOM utils (sorted + real windowing) ===============\n",
    "@contextmanager\n",
    "def dicom_error_handler(fp:str):\n",
    "    try: yield\n",
    "    except Exception as e:\n",
    "        # quiet: keep minimal\n",
    "        raise\n",
    "\n",
    "def _is_dicom_file(p:str)->bool:\n",
    "    try:\n",
    "        with open(p,'rb') as f:\n",
    "            f.seek(128); return f.read(4)==b'DICM'\n",
    "    except: return False\n",
    "\n",
    "def _apply_rescale(img:np.ndarray, ds:pydicom.Dataset)->np.ndarray:\n",
    "    try:\n",
    "        slope = float(getattr(ds,'RescaleSlope',1.0))\n",
    "        intercept = float(getattr(ds,'RescaleIntercept',0.0))\n",
    "        return img*slope + intercept\n",
    "    except: return img\n",
    "\n",
    "def _get_window_from_ds(ds)->Tuple[float,float]:\n",
    "    try:\n",
    "        wc = ds.WindowCenter; ww = ds.WindowWidth\n",
    "        if isinstance(wc, pydicom.multival.MultiValue): wc = float(wc[0])\n",
    "        else: wc = float(wc)\n",
    "        if isinstance(ww, pydicom.multival.MultiValue): ww = float(ww[0])\n",
    "        else: ww = float(ww)\n",
    "        if ww <= 1: ww = 1.0\n",
    "        return wc, ww\n",
    "    except:\n",
    "        modality = getattr(ds,'Modality','CT')\n",
    "        return CFG.windowing_params.get(modality, CFG.windowing_params['default'])\n",
    "\n",
    "def apply_dicom_windowing(img:np.ndarray, center:float, width:float)->np.ndarray:\n",
    "    imin, imax = center - width/2.0, center + width/2.0\n",
    "    img = np.clip(img, imin, imax)\n",
    "    img = (img - imin) / max(imax - imin, 1e-6)\n",
    "    return (img*255).astype(np.uint8)\n",
    "\n",
    "def _process_pixel_array(img:np.ndarray)->np.ndarray:\n",
    "    if img.ndim==3 and img.shape[-1]==3:\n",
    "        img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2GRAY).astype(np.float32)\n",
    "    elif img.ndim==3:  # multi-frame -> middle\n",
    "        img = img[img.shape[0]//2]\n",
    "    elif img.ndim>3:\n",
    "        img = img.reshape(img.shape[-2], img.shape[-1])\n",
    "    return img\n",
    "\n",
    "def process_dicom_series(series_path:str)->Tuple[np.ndarray, Dict]:\n",
    "    series_path = Path(series_path)\n",
    "    files = []\n",
    "    for root,_,fs in os.walk(series_path):\n",
    "        for fn in fs:\n",
    "            p = os.path.join(root, fn)\n",
    "            if fn.lower().endswith(('.dcm','.dicom')) or _is_dicom_file(p):\n",
    "                files.append(p)\n",
    "    if not files:\n",
    "        return _get_default_volume_and_metadata()\n",
    "\n",
    "    # read minimal headers to sort\n",
    "    def sort_key(p):\n",
    "        try:\n",
    "            ds = pydicom.dcmread(p, stop_before_pixels=True, force=True)\n",
    "            ipp = getattr(ds,'ImagePositionPatient', None)\n",
    "            if ipp is not None and len(ipp)>=3:\n",
    "                return float(ipp[2])\n",
    "            return int(getattr(ds,'InstanceNumber',0))\n",
    "        except: return 0\n",
    "    files.sort(key=sort_key)\n",
    "\n",
    "    slices, meta, err = [], {}, 0\n",
    "    for i,p in enumerate(files):\n",
    "        try:\n",
    "            with dicom_error_handler(p):\n",
    "                ds = pydicom.dcmread(p, force=True)\n",
    "                img = _process_pixel_array(ds.pixel_array.astype(np.float32))\n",
    "                if i==0:\n",
    "                    meta = {'modality': getattr(ds,'Modality','CT'),\n",
    "                            'age': _safe_age(ds), 'sex': _safe_sex(ds)}\n",
    "                img = _apply_rescale(img, ds)\n",
    "                if CFG.use_windowing:\n",
    "                    c,w = _get_window_from_ds(ds)\n",
    "                    img = apply_dicom_windowing(img, c, w)\n",
    "                else:\n",
    "                    img = ((img - img.min())/max(img.max()-img.min(),1e-6)*255).astype(np.uint8)\n",
    "                img = cv2.resize(img, (CFG.image_size, CFG.image_size), interpolation=cv2.INTER_LINEAR)\n",
    "                slices.append(img)\n",
    "        except:\n",
    "            err += 1\n",
    "            if err > len(files)*0.5: return _get_default_volume_and_metadata()\n",
    "\n",
    "    if not meta: meta = {'age':50,'sex':0,'modality':'CT'}\n",
    "    volume = _create_volume_from_slices(slices)\n",
    "    return volume, meta\n",
    "\n",
    "def _safe_age(ds):\n",
    "    try:\n",
    "        s = str(getattr(ds,'PatientAge','050Y'))[:3]\n",
    "        d = int(''.join([c for c in s if c.isdigit()]) or 50)\n",
    "        return max(0,min(d,120))\n",
    "    except: return 50\n",
    "\n",
    "def _safe_sex(ds):\n",
    "    try: return 1 if str(getattr(ds,'PatientSex','M')).upper().startswith('M') else 0\n",
    "    except: return 0\n",
    "\n",
    "def _create_volume_from_slices(slices:List[np.ndarray])->np.ndarray:\n",
    "    if not slices:\n",
    "        return np.zeros((CFG.num_slices, CFG.image_size, CFG.image_size), np.uint8)\n",
    "    vol = np.asarray(slices)\n",
    "    n = CFG.num_slices\n",
    "    if len(vol) > n:\n",
    "        idx = np.linspace(0, len(vol)-1, n).astype(int)\n",
    "        vol = vol[idx]\n",
    "    elif len(vol) < n:\n",
    "        pad = n - len(vol)\n",
    "        if len(vol)==1: vol = np.repeat(vol, n, axis=0)\n",
    "        else: vol = np.pad(vol, ((0,pad),(0,0),(0,0)), mode='edge')\n",
    "    return vol\n",
    "\n",
    "def _get_default_volume_and_metadata():\n",
    "    return (np.zeros((CFG.num_slices, CFG.image_size, CFG.image_size), np.uint8),\n",
    "            {'age':50,'sex':0,'modality':'CT'})\n",
    "\n",
    "# =============== Transforms ===============\n",
    "def get_inference_transform():\n",
    "    return A.Compose([A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225], max_pixel_value=255.0),\n",
    "                      ToTensorV2()])\n",
    "def get_tta_transforms():\n",
    "    base = [A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]), ToTensorV2()]\n",
    "    return [\n",
    "        A.Compose(base),\n",
    "        A.Compose([A.HorizontalFlip(p=1.0)]+base),\n",
    "        A.Compose([A.VerticalFlip(p=1.0)]+base),\n",
    "        A.Compose([A.Transpose(p=1.0)]+base),\n",
    "        A.Compose([A.Rotate(limit=10,p=1.0)]+base),\n",
    "        A.Compose([A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=10, p=1.0)]+base),\n",
    "        A.Compose([A.RandomBrightnessContrast(p=1.0)]+base),\n",
    "        A.Compose([A.GaussNoise(var_limit=(5,15), p=1.0)]+base),\n",
    "    ]\n",
    "\n",
    "MODELS: Dict[str, nn.Module] = {}\n",
    "TRANSFORM: Optional[A.Compose] = None\n",
    "TTA_TRANSFORMS: Optional[List[A.Compose]] = None\n",
    "PREDICTION_COUNT = 0\n",
    "\n",
    "# =============== Loading ===============\n",
    "def _validate_model(m:nn.Module):\n",
    "    with torch.no_grad():\n",
    "        im = torch.randn(1,3,CFG.image_size,CFG.image_size, device=device).to(memory_format=torch.channels_last)\n",
    "        me = torch.randn(1,2, device=device)\n",
    "        out = m(im, me)\n",
    "        if out.shape != (1,len(LABEL_COLS)):\n",
    "            raise RuntimeError(f\"bad output {out.shape}\")\n",
    "\n",
    "def load_single_model(name, path)->nn.Module:\n",
    "    if not os.path.exists(path): raise FileNotFoundError(path)\n",
    "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
    "    tr = ckpt.get('training_config', {})\n",
    "    if 'image_size' in tr: CFG.image_size = tr['image_size']\n",
    "    m = MultiBackboneModel(name, num_classes=tr.get('num_classes',14),\n",
    "                           pretrained=False, drop_rate=0.0, drop_path_rate=0.0)\n",
    "    m.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "    m.to(device)\n",
    "    m.eval()\n",
    "    m.to(memory_format=torch.channels_last)\n",
    "    _validate_model(m)\n",
    "    return m\n",
    "\n",
    "def load_models():\n",
    "    global MODELS, TRANSFORM, TTA_TRANSFORMS\n",
    "    if CFG.use_ensemble:\n",
    "        for n,p in MODEL_PATHS.items():\n",
    "            try: MODELS[n] = load_single_model(n,p)\n",
    "            except: pass\n",
    "    else:\n",
    "        n = next(iter(MODEL_PATHS))\n",
    "        MODELS[n] = load_single_model(n, MODEL_PATHS[n])\n",
    "\n",
    "    TRANSFORM = get_inference_transform()\n",
    "    if CFG.use_tta: TTA_TRANSFORMS = get_tta_transforms()\n",
    "    _warmup_models()\n",
    "\n",
    "def _warmup_models():\n",
    "    try:\n",
    "        im = torch.randn(CFG.batch_size,3,CFG.image_size,CFG.image_size, device=device).to(memory_format=torch.channels_last)\n",
    "        me = torch.randn(CFG.batch_size,2, device=device)\n",
    "        with torch.no_grad():\n",
    "            for m in MODELS.values():\n",
    "                for _ in range(2):\n",
    "                    with autocast(enabled=CFG.use_amp):\n",
    "                        _ = m(im, me)\n",
    "        del im, me\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    except: pass\n",
    "\n",
    "# =============== Prediction ===============\n",
    "def _prepare_metadata_tensor(meta:Dict)->torch.Tensor:\n",
    "    age = float(np.clip(meta.get('age',50)/100.0, 0.0, 1.2))\n",
    "    sex = float(np.clip(int(meta.get('sex',0)), 0, 1))\n",
    "    return torch.tensor([[age,sex]], dtype=torch.float32, device=device)\n",
    "\n",
    "def _create_multichannel_input(volume:np.ndarray, center_idx:int)->np.ndarray:\n",
    "    # middle / MIP / STD\n",
    "    mid = np.clip(center_idx, 1, volume.shape[0]-2)\n",
    "    middle = volume[mid]\n",
    "    mip = volume.max(axis=0)\n",
    "    std = volume.astype(np.float32).std(axis=0)\n",
    "    if std.max()>std.min():\n",
    "        std = ((std-std.min())/max(std.max()-std.min(),1e-6)*255).astype(np.uint8)\n",
    "    else:\n",
    "        std = np.full_like(middle, 128, dtype=np.uint8)\n",
    "    img = np.stack([middle, mip, std], -1)\n",
    "    return img\n",
    "\n",
    "def _tta_predict(model, image_np, meta_tensor):\n",
    "    preds = []\n",
    "    if CFG.use_tta and TTA_TRANSFORMS:\n",
    "        for tfm in TTA_TRANSFORMS[:CFG.tta_transforms]:\n",
    "            t = tfm(image=image_np)['image'].unsqueeze(0).to(device, non_blocking=True)\n",
    "            t = t.to(memory_format=torch.channels_last)\n",
    "            with torch.no_grad(), autocast(enabled=CFG.use_amp):\n",
    "                o = model(t, meta_tensor)\n",
    "            preds.append(torch.sigmoid(o).float().cpu().numpy())\n",
    "        return np.mean(preds,0).squeeze()\n",
    "    else:\n",
    "        t = TRANSFORM(image=image_np)['image'].unsqueeze(0).to(device, non_blocking=True)\n",
    "        t = t.to(memory_format=torch.channels_last)\n",
    "        with torch.no_grad(), autocast(enabled=CFG.use_amp):\n",
    "            o = model(t, meta_tensor)\n",
    "        return torch.sigmoid(o).float().cpu().numpy().squeeze()\n",
    "\n",
    "def predict_single_model(model:nn.Module, volume:np.ndarray, meta_tensor:torch.Tensor)->np.ndarray:\n",
    "    # slice bagging around the center\n",
    "    centers = np.linspace(volume.shape[0]//2 - 3, volume.shape[0]//2 + 3, CFG.slice_bags).astype(int)\n",
    "    bag_preds = []\n",
    "    for c in centers:\n",
    "        img = _create_multichannel_input(volume, c)\n",
    "        bag_preds.append(_tta_predict(model, img, meta_tensor))\n",
    "    return np.mean(bag_preds, axis=0)\n",
    "\n",
    "def predict_ensemble(volume:np.ndarray, meta_tensor:torch.Tensor)->np.ndarray:\n",
    "    preds, wts = [], []\n",
    "    for name, m in MODELS.items():\n",
    "        try:\n",
    "            preds.append(predict_single_model(m, volume, meta_tensor))\n",
    "            wts.append(CFG.ensemble_weights.get(name,1.0))\n",
    "        except: pass\n",
    "    if not preds: return np.full(len(LABEL_COLS), 0.1)\n",
    "    wts = np.asarray(wts); wts = wts / wts.sum()\n",
    "    P = np.vstack(preds)\n",
    "    return (P*wts[:,None]).sum(0)\n",
    "\n",
    "def _validate_predictions(p:np.ndarray)->np.ndarray:\n",
    "    if p.shape!=(len(LABEL_COLS),): p = np.resize(p, len(LABEL_COLS))\n",
    "    p = np.nan_to_num(p, nan=0.1, posinf=0.9, neginf=0.0)\n",
    "    return np.clip(p, 1e-3, 1-1e-3)\n",
    "\n",
    "def _manage_memory():\n",
    "    global PREDICTION_COUNT\n",
    "    PREDICTION_COUNT += 1\n",
    "    if CFG.enable_memory_cleanup and (PREDICTION_COUNT % CFG.cleanup_frequency == 0):\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def _predict_inner(series_path:str)->pl.DataFrame:\n",
    "    if not MODELS: load_models()\n",
    "    vol, meta = process_dicom_series(series_path)\n",
    "    meta_t = _prepare_metadata_tensor(meta)\n",
    "    if CFG.use_ensemble and len(MODELS)>1:\n",
    "        pred = predict_ensemble(vol, meta_t)\n",
    "    else:\n",
    "        model = list(MODELS.values())[0]\n",
    "        pred = predict_single_model(model, vol, meta_t)\n",
    "    pred = _validate_predictions(pred)\n",
    "    _manage_memory()\n",
    "    return pl.DataFrame(data=[pred.tolist()], schema=LABEL_COLS, orient='row')\n",
    "\n",
    "def _create_fallback_predictions()->pl.DataFrame:\n",
    "    vals = [0.05]*(len(LABEL_COLS)-1)+[0.1]\n",
    "    return pl.DataFrame(data=[vals], schema=LABEL_COLS, orient='row')\n",
    "\n",
    "def predict(series_path:str)->pl.DataFrame:\n",
    "    try:\n",
    "        if not os.path.exists(series_path):\n",
    "            return _create_fallback_predictions()\n",
    "        return _predict_inner(series_path)\n",
    "    except Exception:\n",
    "        return _create_fallback_predictions()\n",
    "    finally:\n",
    "        try:\n",
    "            shared = '/kaggle/shared'\n",
    "            if os.path.exists(shared): shutil.rmtree(shared, ignore_errors=True)\n",
    "            os.makedirs(shared, exist_ok=True)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache(); torch.cuda.synchronize()\n",
    "            gc.collect()\n",
    "        except: pass\n",
    "\n",
    "# =============== Main (quiet) ===============\n",
    "def main():\n",
    "    load_models()\n",
    "    server = rsna.RSNAInferenceServer(predict)\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        server.serve()\n",
    "    else:\n",
    "        server.run_local_gateway()\n",
    "        try:\n",
    "            sub = pl.read_parquet('/kaggle/working/submission.parquet')\n",
    "            # No verbose prints; keep it lean for Kaggle\n",
    "        except: pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13190393,
     "isSourceIdPinned": false,
     "sourceId": 99552,
     "sourceType": "competition"
    },
    {
     "datasetId": 7976292,
     "sourceId": 12687919,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 97.229236,
   "end_time": "2025-08-09T08:53:46.669358",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-09T08:52:09.440122",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
